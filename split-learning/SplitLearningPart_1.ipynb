{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PXXADNUbuCyl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from uuid import uuid4\n",
    "import syft as sy\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribute_MNIST:\n",
    "    \"\"\"\n",
    "  This class distribute each image among different workers\n",
    "  It returns a dictionary with key as data owner's id and \n",
    "  value as a pointer to the list of data batches at owner's \n",
    "  location.\n",
    "  \n",
    "  example:-  \n",
    "  >>> from distribute_data import Distribute_MNIST\n",
    "  >>> obj = Distribute_MNIST(data_owners= (alice, bob, claire), data_loader= torch.utils.data.DataLoader(trainset)) \n",
    "  >>> obj.data_pointer[1]['alice'].shape, obj.data_pointer[1]['bob'].shape, obj.data_pointer[1]['claire'].shape\n",
    "   (torch.Size([64, 1, 9, 28]),\n",
    "    torch.Size([64, 1, 9, 28]),\n",
    "    torch.Size([64, 1, 10, 28]))\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, data_owners, data_loader):\n",
    "\n",
    "        \"\"\"\n",
    "         Args:\n",
    "          data_owners: tuple of data owners\n",
    "          data_loader: torch.utils.data.DataLoader for MNIST \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_owners = data_owners\n",
    "        self.data_loader = data_loader\n",
    "        self.no_of_owner = len(data_owners)\n",
    "\n",
    "        self.data_pointer = []\n",
    "        \"\"\"\n",
    "        self.data_pointer:  list of dictionaries where \n",
    "        (key, value) = (id of the data holder, a pointer to the list of batches at that data holder).\n",
    "        example:\n",
    "        self.data_pointer  = [\n",
    "                                {\"alice\": pointer_to_alice_batch1, \"bob\": pointer_to_bob_batch1},\n",
    "                                {\"alice\": pointer_to_alice_batch2, \"bob\": pointer_to_bob_batch2},\n",
    "                                ...\n",
    "                             ]\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels = []\n",
    "\n",
    "        # iterate over each batch of dataloader for, 1) spliting image 2) sending to VirtualWorker\n",
    "        for images, labels in self.data_loader:\n",
    "\n",
    "            curr_data_dict = {}\n",
    "\n",
    "            # calculate width and height according to the no. of workers for UNIFORM distribution\n",
    "            height = images.shape[-1]//self.no_of_owner\n",
    "\n",
    "            self.labels.append(labels)\n",
    "\n",
    "            # iterate over each worker for distribution of current batch of the self.data_loader\n",
    "            for i, owner in enumerate(self.data_owners[:-1]):\n",
    "\n",
    "                # split the image and send it to VirtualWorker (which is supposed to be a dataowner or client)\n",
    "                image_part_ptr = images[:, :, :, height * i : height * (i + 1)].send(\n",
    "                    owner\n",
    "                )\n",
    "\n",
    "                curr_data_dict[owner.id] = image_part_ptr\n",
    "\n",
    "            # Repeat same for the remaining part of the image\n",
    "            last_owner = self.data_owners[-1]\n",
    "            last_part_ptr = images[:, :, :, height * (i + 1) :].send(last_owner)\n",
    "\n",
    "            curr_data_dict[last_owner.id] = last_part_ptr\n",
    "\n",
    "            self.data_pointer.append(curr_data_dict)\n",
    "            \n",
    "    def __iter__(self):\n",
    "        \n",
    "        for data_ptr, label in zip(self.data_pointer[:-1], self.labels[:-1]):\n",
    "            yield (data_ptr, label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data_loader)-1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R106TCDKEozj"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "hook = sy.TorchHook(torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "dde44056ded14b1da781f381cd68531c",
      "0ed019f1ca5e44f480a48ed25d1668c0",
      "572768a4ac4640c09b6abb920b64d20c",
      "323542ddb63649fb8fb410e4f7038ac5",
      "3e365a51059d4681983f5894dc733e50",
      "4187546d5cc0494685f29befc0279976",
      "5862037b575c49d384703c30d7103322",
      "59c0dc8a0c7d4b4690436e0bc43de692"
     ]
    },
    "id": "RVhVx_TYEq_O",
    "outputId": "4acc62f5-a917-40be-90fd-98de09442460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10 is a dataset of natural images consisting of 50k training images and 10k test\n",
    "# Every image is labelled with one of the following class\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from FLDataset import CustomCIFARDataset\n",
    "custom_trainset = CustomCIFARDataset(root='./data', train=True,\n",
    "                                        download=True, transform=transform, client_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Assign custom dataset to each client \n",
    "client_trainloader = []\n",
    "for i in range(10):\n",
    "    custoom_data = CustomCIFARDataset(root='./data', train=True,\n",
    "                                        download=True, transform=transform, client_num=i)\n",
    "    client_trainloader.append(\n",
    "    torch.utils.data.DataLoader(custoom_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image[0].shape:  torch.Size([3, 32, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image = custom_trainset[0]\n",
    "print(\"image[0].shape: \", custom_trainset[0][0].shape)\n",
    "imshow(torchvision.utils.make_grid(custom_trainset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = custom_trainset[0]\n",
    "print(\"image[0].shape: \", image[0].shape)\n",
    "# image[0][:, :, height * i : height * (i + 1)].shape\n",
    "height = image[0].shape[-1]//10\n",
    "i = 1\n",
    "newImg = image[0][:, :, height * i : height * (i + 1)]\n",
    "newImg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newImg.shape\n",
    "imshow(torchvision.utils.make_grid(newImg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5   # unnormalize\n",
    "    npimg = img.numpy()   # convert from tensor\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
    "    plt.show()\n",
    "for item in trainloader:\n",
    "    print(item[0][0].shape)\n",
    "    imshow(torchvision.utils.make_grid(item[0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nS8y3XBVE3jx"
   },
   "outputs": [],
   "source": [
    "# Explain nn.Module and explain the forward and backward pass\n",
    "class ResNet18Client(nn.Module):\n",
    "    \"\"\"docstring for ResNet\"\"\"\n",
    "\n",
    "    # Explain initialize (listing the neural network architecture and other related parameters)\n",
    "    def __init__(self, config):\n",
    "        super(ResNet18Client, self).__init__()\n",
    "        # Explain this line\n",
    "        self.cut_layer = config[\"cut_layer\"]\n",
    "\n",
    "        # Explain this line\n",
    "        self.model = models.resnet18(pretrained=False)\n",
    "\n",
    "        self.model = nn.ModuleList(self.model.children())\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    # Explain forward (actually used during the execution of the neural network at runtime)\n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            if i > self.cut_layer:\n",
    "                break\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ih-a3ny-FTiV"
   },
   "outputs": [],
   "source": [
    "class ResNet18Server(nn.Module):\n",
    "    \"\"\"docstring for ResNet\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ResNet18Server, self).__init__()\n",
    "        self.logits = config[\"logits\"]\n",
    "        self.cut_layer = config[\"cut_layer\"]\n",
    "\n",
    "        self.model = models.resnet18(pretrained=False)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        # Explain this part\n",
    "        self.model.fc = nn.Sequential(nn.Flatten(),\n",
    "                                      nn.Linear(num_ftrs, self.logits))\n",
    "\n",
    "        self.model = nn.ModuleList(self.model.children())\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model):\n",
    "            # Explain this part\n",
    "            if i <= self.cut_layer:\n",
    "                continue\n",
    "            x = l(x)\n",
    "        return nn.functional.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWHg9hxZsUHE"
   },
   "source": [
    "### Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RnXG5vGvFVIv"
   },
   "outputs": [],
   "source": [
    "config = {\"cut_layer\": 3, \"logits\": 10}\n",
    "client_model = ResNet18Client(config).to(device)\n",
    "server_model = ResNet18Server(config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQOa25CKsPzc"
   },
   "source": [
    "### Set up the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "k59-mlxvFYlL"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "client_optimizer = optim.SGD(client_model.parameters(), lr=0.01, momentum=0.9)\n",
    "server_optimizer = optim.SGD(server_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuMIR4DasZaS"
   },
   "source": [
    "### Set up the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TgUiA77tKVC"
   },
   "source": [
    "### Add the training diagram and draw forward pass and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "YAdzAnlbFhHV",
    "outputId": "de3bfef6-07b6-421e-b745-e99613053e72",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 200] loss: 2.1173392671346662\n",
      "[2, 200] loss: 1.961812589764595\n",
      "[3, 200] loss: 1.9186802607774736\n",
      "[4, 200] loss: 1.8824517369270324\n",
      "[5, 200] loss: 1.856448255777359\n",
      "[6, 200] loss: 1.842729731798172\n",
      "[7, 200] loss: 1.8166745793819428\n",
      "[8, 200] loss: 1.8020147222280503\n",
      "[9, 200] loss: 1.795249900817871\n",
      "[10, 200] loss: 1.7779013204574585\n",
      "[11, 200] loss: 1.7437717640399932\n",
      "[12, 200] loss: 1.723221452832222\n",
      "[13, 200] loss: 1.7072072249650956\n",
      "[14, 200] loss: 1.6969327634572984\n",
      "[15, 200] loss: 1.6881021469831468\n",
      "[16, 200] loss: 1.6755625301599502\n",
      "[17, 200] loss: 1.6692742383480073\n",
      "[18, 200] loss: 1.6575912821292877\n",
      "[19, 200] loss: 1.64782164812088\n",
      "[20, 200] loss: 1.6421324217319488\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        client_optimizer.zero_grad()\n",
    "        server_optimizer.zero_grad()\n",
    "        \n",
    "        # Client part\n",
    "        activations = client_model(inputs)\n",
    "        server_inputs = activations.detach().clone()\n",
    "        \n",
    "        # Simulation of server part is happening in this portion\n",
    "        # Server part\n",
    "        server_inputs = Variable(server_inputs, requires_grad=True)\n",
    "        outputs = server_model(server_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # server optimization\n",
    "        server_optimizer.step()\n",
    "        \n",
    "        # Simulation of Client Happening in this portion\n",
    "        # Client optimization\n",
    "        activations.backward(server_inputs.grad)\n",
    "        client_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 200 == 199:\n",
    "            print('[{}, {}] loss: {}'.format(epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "h_mLF7ulWTeH"
   },
   "outputs": [],
   "source": [
    "# You can save the model for analyzing it in the future\n",
    "import os\n",
    "current_save_dir = os.getcwd() + '/Experiments/split-learning/saved_models'\n",
    "os.mkdir(current_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aminmoradi/jupyter/openml-fed'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IyFS_0GSFppf"
   },
   "outputs": [],
   "source": [
    "client_model_path = current_save_dir + \"/trained_client_model.pt\"\n",
    "server_model_path = current_save_dir + \"/trained_server_model.pt\"\n",
    "torch.save(client_model.state_dict(), client_model_path)\n",
    "torch.save(server_model.state_dict(), server_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHFMq1kIUYId",
    "outputId": "d4c02965-5139-41ad-d83e-a31c4b221f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6132\n"
     ]
    }
   ],
   "source": [
    "# Mention that accuracy is low because of low number of epochs\n",
    "total, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = server_model(client_model(inputs))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Dg1CR3V8agEf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "clients_hooks = []\n",
    "total_client_num = 10\n",
    "hook = sy.TorchHook(torch)\n",
    "for i in range(total_client_num):\n",
    "    clients_hooks.append(sy.VirtualWorker(hook, id=\"client_\" + str(i)))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize multiple clients\n",
    "client_model_list = []\n",
    "for client_num in range(total_client_num):\n",
    "        model = ResNet18Client(config).to(device)\n",
    "        model.send(clients_hooks[client_num])\n",
    "        client_model_list.append(model)\n",
    "\n",
    "# Initialize multiple optimizers\n",
    "client_optimizer_list = [optim.SGD(client_model_list[client_num].parameters(), lr=0.01, momentum=0.9) for client_num in range(total_client_num)]\n",
    "server = ResNet18Server(config).to(device)\n",
    "server_optimizer = optim.SGD(server.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distributed_trainloader = Distribute_MNIST(data_owners=tuple(clients_hooks), data_loader=trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Assign custom dataset to each client \n",
    "total_client_num = 10\n",
    "client_trainloader = []\n",
    "for i in range(total_client_num):\n",
    "    custoom_data = CustomCIFARDataset(root='./data', train=True,\n",
    "                                        download=True, transform=transform, client_num=i)\n",
    "    client_trainloader.append(\n",
    "    torch.utils.data.DataLoader(custoom_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize multiple clients\n",
    "client_model_list = [ResNet18Client(config).to(device) for client_num in range(total_client_num)]\n",
    "# Initialize multiple optimizers\n",
    "client_optimizer_list = [optim.SGD(client_model_list[client_num].parameters(), lr=0.01, momentum=0.9) for client_num in range(total_client_num)]\n",
    "server = ResNet18Server(config).to(device)\n",
    "server_optimizer = optim.SGD(server.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 1, Loss: 2.2991\n",
      "Epoch: 0, Iteration: 51, Loss: 2.1840\n",
      "Epoch: 0, Iteration: 101, Loss: 2.0722\n",
      "Epoch: 0, Iteration: 151, Loss: 2.1180\n",
      "Epoch: 0, Iteration: 201, Loss: 2.0360\n",
      "Epoch: 0, Iteration: 251, Loss: 2.0200\n",
      "Epoch: 0, Iteration: 301, Loss: 1.9704\n",
      "Epoch: 0, Iteration: 351, Loss: 1.9836\n",
      "Epoch: 1, Iteration: 1, Loss: 1.9568\n",
      "Epoch: 1, Iteration: 51, Loss: 2.0210\n",
      "Epoch: 1, Iteration: 101, Loss: 1.9246\n",
      "Epoch: 1, Iteration: 151, Loss: 1.9311\n",
      "Epoch: 1, Iteration: 201, Loss: 1.9956\n",
      "Epoch: 1, Iteration: 251, Loss: 1.9582\n",
      "Epoch: 1, Iteration: 301, Loss: 1.9665\n",
      "Epoch: 1, Iteration: 351, Loss: 1.8856\n",
      "Epoch: 2, Iteration: 1, Loss: 1.9003\n",
      "Epoch: 2, Iteration: 51, Loss: 1.9121\n",
      "Epoch: 2, Iteration: 101, Loss: 1.8501\n",
      "Epoch: 2, Iteration: 151, Loss: 1.8220\n",
      "Epoch: 2, Iteration: 201, Loss: 1.9426\n",
      "Epoch: 2, Iteration: 251, Loss: 1.8922\n",
      "Epoch: 2, Iteration: 301, Loss: 1.8500\n",
      "Epoch: 2, Iteration: 351, Loss: 1.8729\n",
      "Epoch: 3, Iteration: 1, Loss: 1.8921\n",
      "Epoch: 3, Iteration: 51, Loss: 1.8874\n",
      "Epoch: 3, Iteration: 101, Loss: 1.8488\n",
      "Epoch: 3, Iteration: 151, Loss: 1.7756\n",
      "Epoch: 3, Iteration: 201, Loss: 1.8206\n",
      "Epoch: 3, Iteration: 251, Loss: 1.8952\n",
      "Epoch: 3, Iteration: 301, Loss: 1.8436\n",
      "Epoch: 3, Iteration: 351, Loss: 1.8907\n",
      "Epoch: 4, Iteration: 1, Loss: 1.8560\n",
      "Epoch: 4, Iteration: 51, Loss: 1.8588\n",
      "Epoch: 4, Iteration: 101, Loss: 1.8637\n",
      "Epoch: 4, Iteration: 151, Loss: 1.8034\n",
      "Epoch: 4, Iteration: 201, Loss: 1.8252\n",
      "Epoch: 4, Iteration: 251, Loss: 1.7669\n",
      "Epoch: 4, Iteration: 301, Loss: 1.8010\n",
      "Epoch: 4, Iteration: 351, Loss: 1.8551\n",
      "Epoch: 5, Iteration: 1, Loss: 1.7762\n",
      "Epoch: 5, Iteration: 51, Loss: 1.8378\n",
      "Epoch: 5, Iteration: 101, Loss: 1.7402\n",
      "Epoch: 5, Iteration: 151, Loss: 1.7991\n",
      "Epoch: 5, Iteration: 201, Loss: 1.8283\n",
      "Epoch: 5, Iteration: 251, Loss: 1.8506\n",
      "Epoch: 5, Iteration: 301, Loss: 1.7723\n",
      "Epoch: 5, Iteration: 351, Loss: 1.8444\n",
      "Epoch: 6, Iteration: 1, Loss: 1.7448\n",
      "Epoch: 6, Iteration: 51, Loss: 1.8514\n",
      "Epoch: 6, Iteration: 101, Loss: 1.7456\n",
      "Epoch: 6, Iteration: 151, Loss: 1.8129\n",
      "Epoch: 6, Iteration: 201, Loss: 1.7733\n",
      "Epoch: 6, Iteration: 251, Loss: 1.7733\n",
      "Epoch: 6, Iteration: 301, Loss: 1.7828\n",
      "Epoch: 6, Iteration: 351, Loss: 1.7408\n",
      "Epoch: 7, Iteration: 1, Loss: 1.8132\n",
      "Epoch: 7, Iteration: 51, Loss: 1.7874\n",
      "Epoch: 7, Iteration: 101, Loss: 1.7973\n",
      "Epoch: 7, Iteration: 151, Loss: 1.7134\n",
      "Epoch: 7, Iteration: 201, Loss: 1.8679\n",
      "Epoch: 7, Iteration: 251, Loss: 1.7266\n",
      "Epoch: 7, Iteration: 301, Loss: 1.8196\n",
      "Epoch: 7, Iteration: 351, Loss: 1.7083\n",
      "Epoch: 8, Iteration: 1, Loss: 1.7616\n",
      "Epoch: 8, Iteration: 51, Loss: 1.8013\n",
      "Epoch: 8, Iteration: 101, Loss: 1.7626\n",
      "Epoch: 8, Iteration: 151, Loss: 1.6880\n",
      "Epoch: 8, Iteration: 201, Loss: 1.7518\n",
      "Epoch: 8, Iteration: 251, Loss: 1.7784\n",
      "Epoch: 8, Iteration: 301, Loss: 1.6999\n",
      "Epoch: 8, Iteration: 351, Loss: 1.8080\n",
      "Epoch: 9, Iteration: 1, Loss: 1.6989\n",
      "Epoch: 9, Iteration: 51, Loss: 1.7412\n",
      "Epoch: 9, Iteration: 101, Loss: 1.6738\n",
      "Epoch: 9, Iteration: 151, Loss: 1.6886\n",
      "Epoch: 9, Iteration: 201, Loss: 1.7082\n",
      "Epoch: 9, Iteration: 251, Loss: 1.6399\n",
      "Epoch: 9, Iteration: 301, Loss: 1.7091\n",
      "Epoch: 9, Iteration: 351, Loss: 1.6754\n",
      "Epoch: 10, Iteration: 1, Loss: 1.6950\n",
      "Epoch: 10, Iteration: 51, Loss: 1.8265\n",
      "Epoch: 10, Iteration: 101, Loss: 1.7852\n",
      "Epoch: 10, Iteration: 151, Loss: 1.6470\n",
      "Epoch: 10, Iteration: 201, Loss: 1.6607\n",
      "Epoch: 10, Iteration: 251, Loss: 1.7097\n",
      "Epoch: 10, Iteration: 301, Loss: 1.6606\n",
      "Epoch: 10, Iteration: 351, Loss: 1.7302\n",
      "Epoch: 11, Iteration: 1, Loss: 1.7271\n",
      "Epoch: 11, Iteration: 51, Loss: 1.7189\n",
      "Epoch: 11, Iteration: 101, Loss: 1.6687\n",
      "Epoch: 11, Iteration: 151, Loss: 1.7046\n",
      "Epoch: 11, Iteration: 201, Loss: 1.6849\n",
      "Epoch: 11, Iteration: 251, Loss: 1.7073\n",
      "Epoch: 11, Iteration: 301, Loss: 1.6546\n",
      "Epoch: 11, Iteration: 351, Loss: 1.7277\n",
      "Epoch: 12, Iteration: 1, Loss: 1.7069\n",
      "Epoch: 12, Iteration: 51, Loss: 1.7038\n",
      "Epoch: 12, Iteration: 101, Loss: 1.7026\n",
      "Epoch: 12, Iteration: 151, Loss: 1.7131\n",
      "Epoch: 12, Iteration: 201, Loss: 1.7213\n",
      "Epoch: 12, Iteration: 251, Loss: 1.6849\n",
      "Epoch: 12, Iteration: 301, Loss: 1.6394\n",
      "Epoch: 12, Iteration: 351, Loss: 1.6693\n",
      "Epoch: 13, Iteration: 1, Loss: 1.6292\n",
      "Epoch: 13, Iteration: 51, Loss: 1.6390\n",
      "Epoch: 13, Iteration: 101, Loss: 1.6243\n",
      "Epoch: 13, Iteration: 151, Loss: 1.6584\n",
      "Epoch: 13, Iteration: 201, Loss: 1.6439\n",
      "Epoch: 13, Iteration: 251, Loss: 1.7260\n",
      "Epoch: 13, Iteration: 301, Loss: 1.7233\n",
      "Epoch: 13, Iteration: 351, Loss: 1.7194\n",
      "Epoch: 14, Iteration: 1, Loss: 1.6200\n",
      "Epoch: 14, Iteration: 51, Loss: 1.6217\n",
      "Epoch: 14, Iteration: 101, Loss: 1.7813\n",
      "Epoch: 14, Iteration: 151, Loss: 1.6922\n",
      "Epoch: 14, Iteration: 201, Loss: 1.7299\n",
      "Epoch: 14, Iteration: 251, Loss: 1.6992\n",
      "Epoch: 14, Iteration: 301, Loss: 1.6665\n",
      "Epoch: 14, Iteration: 351, Loss: 1.6553\n",
      "Epoch: 15, Iteration: 1, Loss: 1.6390\n",
      "Epoch: 15, Iteration: 51, Loss: 1.6781\n",
      "Epoch: 15, Iteration: 101, Loss: 1.7436\n",
      "Epoch: 15, Iteration: 151, Loss: 1.7529\n",
      "Epoch: 15, Iteration: 201, Loss: 1.6651\n",
      "Epoch: 15, Iteration: 251, Loss: 1.6617\n",
      "Epoch: 15, Iteration: 301, Loss: 1.6675\n",
      "Epoch: 15, Iteration: 351, Loss: 1.6803\n",
      "Epoch: 16, Iteration: 1, Loss: 1.6394\n",
      "Epoch: 16, Iteration: 51, Loss: 1.6812\n",
      "Epoch: 16, Iteration: 101, Loss: 1.6350\n",
      "Epoch: 16, Iteration: 151, Loss: 1.6684\n",
      "Epoch: 16, Iteration: 201, Loss: 1.6785\n",
      "Epoch: 16, Iteration: 251, Loss: 1.6184\n",
      "Epoch: 16, Iteration: 301, Loss: 1.7220\n",
      "Epoch: 16, Iteration: 351, Loss: 1.6978\n",
      "Epoch: 17, Iteration: 1, Loss: 1.6372\n",
      "Epoch: 17, Iteration: 51, Loss: 1.6691\n",
      "Epoch: 17, Iteration: 101, Loss: 1.6495\n",
      "Epoch: 17, Iteration: 151, Loss: 1.6423\n",
      "Epoch: 17, Iteration: 201, Loss: 1.6569\n",
      "Epoch: 17, Iteration: 251, Loss: 1.6349\n",
      "Epoch: 17, Iteration: 301, Loss: 1.7013\n",
      "Epoch: 17, Iteration: 351, Loss: 1.6771\n",
      "Epoch: 18, Iteration: 1, Loss: 1.6576\n",
      "Epoch: 18, Iteration: 51, Loss: 1.6048\n",
      "Epoch: 18, Iteration: 101, Loss: 1.6283\n",
      "Epoch: 18, Iteration: 151, Loss: 1.6289\n",
      "Epoch: 18, Iteration: 201, Loss: 1.6211\n",
      "Epoch: 18, Iteration: 251, Loss: 1.6313\n",
      "Epoch: 18, Iteration: 301, Loss: 1.7180\n",
      "Epoch: 18, Iteration: 351, Loss: 1.6428\n",
      "Epoch: 19, Iteration: 1, Loss: 1.6474\n",
      "Epoch: 19, Iteration: 51, Loss: 1.6480\n",
      "Epoch: 19, Iteration: 101, Loss: 1.6158\n",
      "Epoch: 19, Iteration: 151, Loss: 1.6470\n",
      "Epoch: 19, Iteration: 201, Loss: 1.6826\n",
      "Epoch: 19, Iteration: 251, Loss: 1.7353\n",
      "Epoch: 19, Iteration: 301, Loss: 1.6360\n",
      "Epoch: 19, Iteration: 351, Loss: 1.6788\n",
      "Epoch: 20, Iteration: 1, Loss: 1.5816\n",
      "Epoch: 20, Iteration: 51, Loss: 1.5860\n",
      "Epoch: 20, Iteration: 101, Loss: 1.6847\n",
      "Epoch: 20, Iteration: 151, Loss: 1.6311\n",
      "Epoch: 20, Iteration: 201, Loss: 1.6858\n",
      "Epoch: 20, Iteration: 251, Loss: 1.6290\n",
      "Epoch: 20, Iteration: 301, Loss: 1.6434\n",
      "Epoch: 20, Iteration: 351, Loss: 1.6242\n",
      "Epoch: 21, Iteration: 1, Loss: 1.6078\n",
      "Epoch: 21, Iteration: 51, Loss: 1.5936\n",
      "Epoch: 21, Iteration: 101, Loss: 1.6173\n",
      "Epoch: 21, Iteration: 151, Loss: 1.6409\n",
      "Epoch: 21, Iteration: 201, Loss: 1.6146\n",
      "Epoch: 21, Iteration: 251, Loss: 1.6025\n",
      "Epoch: 21, Iteration: 301, Loss: 1.5750\n",
      "Epoch: 21, Iteration: 351, Loss: 1.6577\n",
      "Epoch: 22, Iteration: 1, Loss: 1.5983\n",
      "Epoch: 22, Iteration: 51, Loss: 1.6650\n",
      "Epoch: 22, Iteration: 101, Loss: 1.5816\n",
      "Epoch: 22, Iteration: 151, Loss: 1.6322\n",
      "Epoch: 22, Iteration: 201, Loss: 1.6316\n",
      "Epoch: 22, Iteration: 251, Loss: 1.5913\n",
      "Epoch: 22, Iteration: 301, Loss: 1.6023\n",
      "Epoch: 22, Iteration: 351, Loss: 1.6034\n",
      "Epoch: 23, Iteration: 1, Loss: 1.5586\n",
      "Epoch: 23, Iteration: 51, Loss: 1.6096\n",
      "Epoch: 23, Iteration: 101, Loss: 1.6127\n",
      "Epoch: 23, Iteration: 151, Loss: 1.5998\n",
      "Epoch: 23, Iteration: 201, Loss: 1.6045\n",
      "Epoch: 23, Iteration: 251, Loss: 1.6338\n",
      "Epoch: 23, Iteration: 301, Loss: 1.5836\n",
      "Epoch: 23, Iteration: 351, Loss: 1.5828\n",
      "Epoch: 24, Iteration: 1, Loss: 1.6296\n",
      "Epoch: 24, Iteration: 51, Loss: 1.6458\n",
      "Epoch: 24, Iteration: 101, Loss: 1.6211\n",
      "Epoch: 24, Iteration: 151, Loss: 1.5811\n",
      "Epoch: 24, Iteration: 201, Loss: 1.6136\n",
      "Epoch: 24, Iteration: 251, Loss: 1.6384\n",
      "Epoch: 24, Iteration: 301, Loss: 1.6054\n",
      "Epoch: 24, Iteration: 351, Loss: 1.6156\n",
      "Epoch: 25, Iteration: 1, Loss: 1.5684\n",
      "Epoch: 25, Iteration: 51, Loss: 1.6337\n",
      "Epoch: 25, Iteration: 101, Loss: 1.5727\n",
      "Epoch: 25, Iteration: 151, Loss: 1.6022\n",
      "Epoch: 25, Iteration: 201, Loss: 1.6378\n",
      "Epoch: 25, Iteration: 251, Loss: 1.5834\n",
      "Epoch: 25, Iteration: 301, Loss: 1.6201\n",
      "Epoch: 25, Iteration: 351, Loss: 1.5967\n",
      "Epoch: 26, Iteration: 1, Loss: 1.6088\n",
      "Epoch: 26, Iteration: 51, Loss: 1.6231\n",
      "Epoch: 26, Iteration: 101, Loss: 1.5401\n",
      "Epoch: 26, Iteration: 151, Loss: 1.6082\n",
      "Epoch: 26, Iteration: 201, Loss: 1.5972\n",
      "Epoch: 26, Iteration: 251, Loss: 1.5944\n",
      "Epoch: 26, Iteration: 301, Loss: 1.6299\n",
      "Epoch: 26, Iteration: 351, Loss: 1.5409\n",
      "Epoch: 27, Iteration: 1, Loss: 1.6098\n",
      "Epoch: 27, Iteration: 51, Loss: 1.5739\n",
      "Epoch: 27, Iteration: 101, Loss: 1.6066\n",
      "Epoch: 27, Iteration: 151, Loss: 1.6033\n",
      "Epoch: 27, Iteration: 201, Loss: 1.5920\n",
      "Epoch: 27, Iteration: 251, Loss: 1.5582\n",
      "Epoch: 27, Iteration: 301, Loss: 1.5840\n",
      "Epoch: 27, Iteration: 351, Loss: 1.5401\n",
      "Epoch: 28, Iteration: 1, Loss: 1.5871\n",
      "Epoch: 28, Iteration: 51, Loss: 1.6079\n",
      "Epoch: 28, Iteration: 101, Loss: 1.6214\n",
      "Epoch: 28, Iteration: 151, Loss: 1.5465\n",
      "Epoch: 28, Iteration: 201, Loss: 1.6110\n",
      "Epoch: 28, Iteration: 251, Loss: 1.5776\n",
      "Epoch: 28, Iteration: 301, Loss: 1.5851\n",
      "Epoch: 28, Iteration: 351, Loss: 1.5572\n",
      "Epoch: 29, Iteration: 1, Loss: 1.5518\n",
      "Epoch: 29, Iteration: 51, Loss: 1.5578\n",
      "Epoch: 29, Iteration: 101, Loss: 1.5681\n",
      "Epoch: 29, Iteration: 151, Loss: 1.5656\n",
      "Epoch: 29, Iteration: 201, Loss: 1.6059\n",
      "Epoch: 29, Iteration: 251, Loss: 1.5239\n",
      "Epoch: 29, Iteration: 301, Loss: 1.5029\n",
      "Epoch: 29, Iteration: 351, Loss: 1.6103\n",
      "Epoch: 30, Iteration: 1, Loss: 1.5722\n",
      "Epoch: 30, Iteration: 51, Loss: 1.5636\n",
      "Epoch: 30, Iteration: 101, Loss: 1.5766\n",
      "Epoch: 30, Iteration: 151, Loss: 1.5508\n",
      "Epoch: 30, Iteration: 201, Loss: 1.5609\n",
      "Epoch: 30, Iteration: 251, Loss: 1.5859\n",
      "Epoch: 30, Iteration: 301, Loss: 1.5588\n",
      "Epoch: 30, Iteration: 351, Loss: 1.5831\n",
      "Epoch: 31, Iteration: 1, Loss: 1.5586\n",
      "Epoch: 31, Iteration: 51, Loss: 1.5860\n",
      "Epoch: 31, Iteration: 101, Loss: 1.5462\n",
      "Epoch: 31, Iteration: 151, Loss: 1.5633\n",
      "Epoch: 31, Iteration: 201, Loss: 1.5445\n",
      "Epoch: 31, Iteration: 251, Loss: 1.5470\n",
      "Epoch: 31, Iteration: 301, Loss: 1.5301\n",
      "Epoch: 31, Iteration: 351, Loss: 1.5429\n",
      "Epoch: 32, Iteration: 1, Loss: 1.6002\n",
      "Epoch: 32, Iteration: 51, Loss: 1.5493\n",
      "Epoch: 32, Iteration: 101, Loss: 1.5789\n",
      "Epoch: 32, Iteration: 151, Loss: 1.5706\n",
      "Epoch: 32, Iteration: 201, Loss: 1.5571\n",
      "Epoch: 32, Iteration: 251, Loss: 1.5708\n",
      "Epoch: 32, Iteration: 301, Loss: 1.5797\n",
      "Epoch: 32, Iteration: 351, Loss: 1.5430\n",
      "Epoch: 33, Iteration: 1, Loss: 1.5284\n",
      "Epoch: 33, Iteration: 51, Loss: 1.5300\n",
      "Epoch: 33, Iteration: 101, Loss: 1.6107\n",
      "Epoch: 33, Iteration: 151, Loss: 1.5938\n",
      "Epoch: 33, Iteration: 201, Loss: 1.5131\n",
      "Epoch: 33, Iteration: 251, Loss: 1.5397\n",
      "Epoch: 33, Iteration: 301, Loss: 1.5866\n",
      "Epoch: 33, Iteration: 351, Loss: 1.5205\n",
      "Epoch: 34, Iteration: 1, Loss: 1.5600\n",
      "Epoch: 34, Iteration: 51, Loss: 1.5870\n",
      "Epoch: 34, Iteration: 101, Loss: 1.5782\n",
      "Epoch: 34, Iteration: 151, Loss: 1.5913\n",
      "Epoch: 34, Iteration: 201, Loss: 1.5190\n",
      "Epoch: 34, Iteration: 251, Loss: 1.5602\n",
      "Epoch: 34, Iteration: 301, Loss: 1.5217\n",
      "Epoch: 34, Iteration: 351, Loss: 1.5325\n",
      "Epoch: 35, Iteration: 1, Loss: 1.5278\n",
      "Epoch: 35, Iteration: 51, Loss: 1.5286\n",
      "Epoch: 35, Iteration: 101, Loss: 1.5418\n",
      "Epoch: 35, Iteration: 151, Loss: 1.5190\n",
      "Epoch: 35, Iteration: 201, Loss: 1.5527\n",
      "Epoch: 35, Iteration: 251, Loss: 1.5674\n",
      "Epoch: 35, Iteration: 301, Loss: 1.5474\n",
      "Epoch: 35, Iteration: 351, Loss: 1.5868\n",
      "Epoch: 36, Iteration: 1, Loss: 1.5404\n",
      "Epoch: 36, Iteration: 51, Loss: 1.5151\n",
      "Epoch: 36, Iteration: 101, Loss: 1.5112\n",
      "Epoch: 36, Iteration: 151, Loss: 1.5670\n",
      "Epoch: 36, Iteration: 201, Loss: 1.5731\n",
      "Epoch: 36, Iteration: 251, Loss: 1.5433\n",
      "Epoch: 36, Iteration: 301, Loss: 1.5323\n",
      "Epoch: 36, Iteration: 351, Loss: 1.5423\n",
      "Epoch: 37, Iteration: 1, Loss: 1.5495\n",
      "Epoch: 37, Iteration: 51, Loss: 1.5556\n",
      "Epoch: 37, Iteration: 101, Loss: 1.5336\n",
      "Epoch: 37, Iteration: 151, Loss: 1.5433\n",
      "Epoch: 37, Iteration: 201, Loss: 1.5711\n",
      "Epoch: 37, Iteration: 251, Loss: 1.5789\n",
      "Epoch: 37, Iteration: 301, Loss: 1.5511\n",
      "Epoch: 37, Iteration: 351, Loss: 1.5399\n",
      "Epoch: 38, Iteration: 1, Loss: 1.5681\n",
      "Epoch: 38, Iteration: 51, Loss: 1.5272\n",
      "Epoch: 38, Iteration: 101, Loss: 1.5237\n",
      "Epoch: 38, Iteration: 151, Loss: 1.5389\n",
      "Epoch: 38, Iteration: 201, Loss: 1.5622\n",
      "Epoch: 38, Iteration: 251, Loss: 1.5783\n",
      "Epoch: 38, Iteration: 301, Loss: 1.5370\n",
      "Epoch: 38, Iteration: 351, Loss: 1.5354\n",
      "Epoch: 39, Iteration: 1, Loss: 1.5413\n",
      "Epoch: 39, Iteration: 51, Loss: 1.5411\n",
      "Epoch: 39, Iteration: 101, Loss: 1.5540\n",
      "Epoch: 39, Iteration: 151, Loss: 1.5414\n",
      "Epoch: 39, Iteration: 201, Loss: 1.5240\n",
      "Epoch: 39, Iteration: 251, Loss: 1.5570\n",
      "Epoch: 39, Iteration: 301, Loss: 1.4912\n",
      "Epoch: 39, Iteration: 351, Loss: 1.5330\n",
      "Epoch: 40, Iteration: 1, Loss: 1.5258\n",
      "Epoch: 40, Iteration: 51, Loss: 1.5447\n",
      "Epoch: 40, Iteration: 101, Loss: 1.5756\n",
      "Epoch: 40, Iteration: 151, Loss: 1.5401\n",
      "Epoch: 40, Iteration: 201, Loss: 1.5423\n",
      "Epoch: 40, Iteration: 251, Loss: 1.5230\n",
      "Epoch: 40, Iteration: 301, Loss: 1.5665\n",
      "Epoch: 40, Iteration: 351, Loss: 1.5005\n",
      "Epoch: 41, Iteration: 1, Loss: 1.5193\n",
      "Epoch: 41, Iteration: 51, Loss: 1.5086\n",
      "Epoch: 41, Iteration: 101, Loss: 1.5400\n",
      "Epoch: 41, Iteration: 151, Loss: 1.5731\n",
      "Epoch: 41, Iteration: 201, Loss: 1.5786\n",
      "Epoch: 41, Iteration: 251, Loss: 1.5179\n",
      "Epoch: 41, Iteration: 301, Loss: 1.5226\n",
      "Epoch: 41, Iteration: 351, Loss: 1.5322\n",
      "Epoch: 42, Iteration: 1, Loss: 1.5537\n",
      "Epoch: 42, Iteration: 51, Loss: 1.5257\n",
      "Epoch: 42, Iteration: 101, Loss: 1.5276\n",
      "Epoch: 42, Iteration: 151, Loss: 1.5268\n",
      "Epoch: 42, Iteration: 201, Loss: 1.5191\n",
      "Epoch: 42, Iteration: 251, Loss: 1.5168\n",
      "Epoch: 42, Iteration: 301, Loss: 1.4804\n",
      "Epoch: 42, Iteration: 351, Loss: 1.5267\n",
      "Epoch: 43, Iteration: 1, Loss: 1.5534\n",
      "Epoch: 43, Iteration: 51, Loss: 1.5592\n",
      "Epoch: 43, Iteration: 101, Loss: 1.5411\n",
      "Epoch: 43, Iteration: 151, Loss: 1.5204\n",
      "Epoch: 43, Iteration: 201, Loss: 1.5435\n",
      "Epoch: 43, Iteration: 251, Loss: 1.5191\n",
      "Epoch: 43, Iteration: 301, Loss: 1.5621\n",
      "Epoch: 43, Iteration: 351, Loss: 1.5400\n",
      "Epoch: 44, Iteration: 1, Loss: 1.5453\n",
      "Epoch: 44, Iteration: 51, Loss: 1.4777\n",
      "Epoch: 44, Iteration: 101, Loss: 1.5324\n",
      "Epoch: 44, Iteration: 151, Loss: 1.5397\n",
      "Epoch: 44, Iteration: 201, Loss: 1.5289\n",
      "Epoch: 44, Iteration: 251, Loss: 1.5205\n",
      "Epoch: 44, Iteration: 301, Loss: 1.5000\n",
      "Epoch: 44, Iteration: 351, Loss: 1.5306\n",
      "Epoch: 45, Iteration: 1, Loss: 1.5197\n",
      "Epoch: 45, Iteration: 51, Loss: 1.5326\n",
      "Epoch: 45, Iteration: 101, Loss: 1.5226\n",
      "Epoch: 45, Iteration: 151, Loss: 1.5546\n",
      "Epoch: 45, Iteration: 201, Loss: 1.4976\n",
      "Epoch: 45, Iteration: 251, Loss: 1.5394\n",
      "Epoch: 45, Iteration: 301, Loss: 1.5009\n",
      "Epoch: 45, Iteration: 351, Loss: 1.5209\n",
      "Epoch: 46, Iteration: 1, Loss: 1.5246\n",
      "Epoch: 46, Iteration: 51, Loss: 1.5256\n",
      "Epoch: 46, Iteration: 101, Loss: 1.5157\n",
      "Epoch: 46, Iteration: 151, Loss: 1.5110\n",
      "Epoch: 46, Iteration: 201, Loss: 1.5020\n",
      "Epoch: 46, Iteration: 251, Loss: 1.5030\n",
      "Epoch: 46, Iteration: 301, Loss: 1.5362\n",
      "Epoch: 46, Iteration: 351, Loss: 1.5149\n",
      "Epoch: 47, Iteration: 1, Loss: 1.5032\n",
      "Epoch: 47, Iteration: 51, Loss: 1.5416\n",
      "Epoch: 47, Iteration: 101, Loss: 1.5274\n",
      "Epoch: 47, Iteration: 151, Loss: 1.5442\n",
      "Epoch: 47, Iteration: 201, Loss: 1.5045\n",
      "Epoch: 47, Iteration: 251, Loss: 1.5110\n",
      "Epoch: 47, Iteration: 301, Loss: 1.5426\n",
      "Epoch: 47, Iteration: 351, Loss: 1.5379\n",
      "Epoch: 48, Iteration: 1, Loss: 1.5200\n",
      "Epoch: 48, Iteration: 51, Loss: 1.5156\n",
      "Epoch: 48, Iteration: 101, Loss: 1.5244\n",
      "Epoch: 48, Iteration: 151, Loss: 1.5311\n",
      "Epoch: 48, Iteration: 201, Loss: 1.5292\n",
      "Epoch: 48, Iteration: 251, Loss: 1.4996\n",
      "Epoch: 48, Iteration: 301, Loss: 1.5699\n",
      "Epoch: 48, Iteration: 351, Loss: 1.5408\n",
      "Epoch: 49, Iteration: 1, Loss: 1.5370\n",
      "Epoch: 49, Iteration: 51, Loss: 1.5288\n",
      "Epoch: 49, Iteration: 101, Loss: 1.5052\n",
      "Epoch: 49, Iteration: 151, Loss: 1.4784\n",
      "Epoch: 49, Iteration: 201, Loss: 1.5669\n",
      "Epoch: 49, Iteration: 251, Loss: 1.5188\n",
      "Epoch: 49, Iteration: 301, Loss: 1.5287\n",
      "Epoch: 49, Iteration: 351, Loss: 1.5030\n",
      "Epoch: 50, Iteration: 1, Loss: 1.5451\n",
      "Epoch: 50, Iteration: 51, Loss: 1.4839\n",
      "Epoch: 50, Iteration: 101, Loss: 1.4818\n",
      "Epoch: 50, Iteration: 151, Loss: 1.5080\n",
      "Epoch: 50, Iteration: 201, Loss: 1.5251\n",
      "Epoch: 50, Iteration: 251, Loss: 1.5016\n",
      "Epoch: 50, Iteration: 301, Loss: 1.5146\n",
      "Epoch: 50, Iteration: 351, Loss: 1.5426\n",
      "Epoch: 51, Iteration: 1, Loss: 1.5070\n",
      "Epoch: 51, Iteration: 51, Loss: 1.5187\n",
      "Epoch: 51, Iteration: 101, Loss: 1.5085\n",
      "Epoch: 51, Iteration: 151, Loss: 1.5162\n",
      "Epoch: 51, Iteration: 201, Loss: 1.5295\n",
      "Epoch: 51, Iteration: 251, Loss: 1.5167\n",
      "Epoch: 51, Iteration: 301, Loss: 1.5123\n",
      "Epoch: 51, Iteration: 351, Loss: 1.5390\n",
      "Epoch: 52, Iteration: 1, Loss: 1.5414\n",
      "Epoch: 52, Iteration: 51, Loss: 1.5125\n",
      "Epoch: 52, Iteration: 101, Loss: 1.5366\n",
      "Epoch: 52, Iteration: 151, Loss: 1.5005\n",
      "Epoch: 52, Iteration: 201, Loss: 1.5003\n",
      "Epoch: 52, Iteration: 251, Loss: 1.5075\n",
      "Epoch: 52, Iteration: 301, Loss: 1.5074\n",
      "Epoch: 52, Iteration: 351, Loss: 1.4870\n",
      "Epoch: 53, Iteration: 1, Loss: 1.5097\n",
      "Epoch: 53, Iteration: 51, Loss: 1.4982\n",
      "Epoch: 53, Iteration: 101, Loss: 1.5319\n",
      "Epoch: 53, Iteration: 151, Loss: 1.4912\n",
      "Epoch: 53, Iteration: 201, Loss: 1.4949\n",
      "Epoch: 53, Iteration: 251, Loss: 1.5040\n",
      "Epoch: 53, Iteration: 301, Loss: 1.4782\n",
      "Epoch: 53, Iteration: 351, Loss: 1.5500\n",
      "Epoch: 54, Iteration: 1, Loss: 1.4987\n",
      "Epoch: 54, Iteration: 51, Loss: 1.5558\n",
      "Epoch: 54, Iteration: 101, Loss: 1.5477\n",
      "Epoch: 54, Iteration: 151, Loss: 1.5079\n",
      "Epoch: 54, Iteration: 201, Loss: 1.5067\n",
      "Epoch: 54, Iteration: 251, Loss: 1.5650\n",
      "Epoch: 54, Iteration: 301, Loss: 1.5103\n",
      "Epoch: 54, Iteration: 351, Loss: 1.5089\n",
      "Epoch: 55, Iteration: 1, Loss: 1.5083\n",
      "Epoch: 55, Iteration: 51, Loss: 1.5147\n",
      "Epoch: 55, Iteration: 101, Loss: 1.5324\n",
      "Epoch: 55, Iteration: 151, Loss: 1.5223\n",
      "Epoch: 55, Iteration: 201, Loss: 1.5018\n",
      "Epoch: 55, Iteration: 251, Loss: 1.5003\n",
      "Epoch: 55, Iteration: 301, Loss: 1.5229\n",
      "Epoch: 55, Iteration: 351, Loss: 1.5093\n",
      "Epoch: 56, Iteration: 1, Loss: 1.5052\n",
      "Epoch: 56, Iteration: 51, Loss: 1.5210\n",
      "Epoch: 56, Iteration: 101, Loss: 1.5065\n",
      "Epoch: 56, Iteration: 151, Loss: 1.5006\n",
      "Epoch: 56, Iteration: 201, Loss: 1.5306\n",
      "Epoch: 56, Iteration: 251, Loss: 1.5004\n",
      "Epoch: 56, Iteration: 301, Loss: 1.4849\n",
      "Epoch: 56, Iteration: 351, Loss: 1.5787\n",
      "Epoch: 57, Iteration: 1, Loss: 1.4863\n",
      "Epoch: 57, Iteration: 51, Loss: 1.5155\n",
      "Epoch: 57, Iteration: 101, Loss: 1.4982\n",
      "Epoch: 57, Iteration: 151, Loss: 1.4928\n",
      "Epoch: 57, Iteration: 201, Loss: 1.4666\n",
      "Epoch: 57, Iteration: 251, Loss: 1.5004\n",
      "Epoch: 57, Iteration: 301, Loss: 1.4929\n",
      "Epoch: 57, Iteration: 351, Loss: 1.5078\n",
      "Epoch: 58, Iteration: 1, Loss: 1.5258\n",
      "Epoch: 58, Iteration: 51, Loss: 1.5217\n",
      "Epoch: 58, Iteration: 101, Loss: 1.5193\n",
      "Epoch: 58, Iteration: 151, Loss: 1.4944\n",
      "Epoch: 58, Iteration: 201, Loss: 1.4851\n",
      "Epoch: 58, Iteration: 251, Loss: 1.5237\n",
      "Epoch: 58, Iteration: 301, Loss: 1.5609\n",
      "Epoch: 58, Iteration: 351, Loss: 1.4929\n",
      "Epoch: 59, Iteration: 1, Loss: 1.5009\n",
      "Epoch: 59, Iteration: 51, Loss: 1.5533\n",
      "Epoch: 59, Iteration: 101, Loss: 1.5105\n",
      "Epoch: 59, Iteration: 151, Loss: 1.5009\n",
      "Epoch: 59, Iteration: 201, Loss: 1.5166\n",
      "Epoch: 59, Iteration: 251, Loss: 1.5098\n",
      "Epoch: 59, Iteration: 301, Loss: 1.5234\n",
      "Epoch: 59, Iteration: 351, Loss: 1.4849\n",
      "Epoch: 60, Iteration: 1, Loss: 1.5031\n",
      "Epoch: 60, Iteration: 51, Loss: 1.5192\n",
      "Epoch: 60, Iteration: 101, Loss: 1.5085\n",
      "Epoch: 60, Iteration: 151, Loss: 1.4847\n",
      "Epoch: 60, Iteration: 201, Loss: 1.4881\n",
      "Epoch: 60, Iteration: 251, Loss: 1.5033\n",
      "Epoch: 60, Iteration: 301, Loss: 1.5156\n",
      "Epoch: 60, Iteration: 351, Loss: 1.4891\n",
      "Epoch: 61, Iteration: 1, Loss: 1.4954\n",
      "Epoch: 61, Iteration: 51, Loss: 1.4801\n",
      "Epoch: 61, Iteration: 101, Loss: 1.4950\n",
      "Epoch: 61, Iteration: 151, Loss: 1.5059\n",
      "Epoch: 61, Iteration: 201, Loss: 1.4999\n",
      "Epoch: 61, Iteration: 251, Loss: 1.5028\n",
      "Epoch: 61, Iteration: 301, Loss: 1.5206\n",
      "Epoch: 61, Iteration: 351, Loss: 1.4997\n",
      "Epoch: 62, Iteration: 1, Loss: 1.4952\n",
      "Epoch: 62, Iteration: 51, Loss: 1.5232\n",
      "Epoch: 62, Iteration: 101, Loss: 1.5007\n",
      "Epoch: 62, Iteration: 151, Loss: 1.5177\n",
      "Epoch: 62, Iteration: 201, Loss: 1.5239\n",
      "Epoch: 62, Iteration: 251, Loss: 1.5147\n",
      "Epoch: 62, Iteration: 301, Loss: 1.4927\n",
      "Epoch: 62, Iteration: 351, Loss: 1.4955\n",
      "Epoch: 63, Iteration: 1, Loss: 1.5317\n",
      "Epoch: 63, Iteration: 51, Loss: 1.4858\n",
      "Epoch: 63, Iteration: 101, Loss: 1.4932\n",
      "Epoch: 63, Iteration: 151, Loss: 1.5099\n",
      "Epoch: 63, Iteration: 201, Loss: 1.5088\n",
      "Epoch: 63, Iteration: 251, Loss: 1.4930\n",
      "Epoch: 63, Iteration: 301, Loss: 1.5327\n",
      "Epoch: 63, Iteration: 351, Loss: 1.5038\n",
      "Epoch: 64, Iteration: 1, Loss: 1.4776\n",
      "Epoch: 64, Iteration: 51, Loss: 1.5477\n",
      "Epoch: 64, Iteration: 101, Loss: 1.4770\n",
      "Epoch: 64, Iteration: 151, Loss: 1.5008\n",
      "Epoch: 64, Iteration: 201, Loss: 1.5065\n",
      "Epoch: 64, Iteration: 251, Loss: 1.5167\n",
      "Epoch: 64, Iteration: 301, Loss: 1.5154\n",
      "Epoch: 64, Iteration: 351, Loss: 1.4940\n",
      "Epoch: 65, Iteration: 1, Loss: 1.5083\n",
      "Epoch: 65, Iteration: 51, Loss: 1.5105\n",
      "Epoch: 65, Iteration: 101, Loss: 1.5099\n",
      "Epoch: 65, Iteration: 151, Loss: 1.5235\n",
      "Epoch: 65, Iteration: 201, Loss: 1.4931\n",
      "Epoch: 65, Iteration: 251, Loss: 1.5047\n",
      "Epoch: 65, Iteration: 301, Loss: 1.5232\n",
      "Epoch: 65, Iteration: 351, Loss: 1.5314\n",
      "Epoch: 66, Iteration: 1, Loss: 1.4926\n",
      "Epoch: 66, Iteration: 51, Loss: 1.4875\n",
      "Epoch: 66, Iteration: 101, Loss: 1.5080\n",
      "Epoch: 66, Iteration: 151, Loss: 1.5004\n",
      "Epoch: 66, Iteration: 201, Loss: 1.4948\n",
      "Epoch: 66, Iteration: 251, Loss: 1.4950\n",
      "Epoch: 66, Iteration: 301, Loss: 1.4776\n",
      "Epoch: 66, Iteration: 351, Loss: 1.5001\n",
      "Epoch: 67, Iteration: 1, Loss: 1.5245\n",
      "Epoch: 67, Iteration: 51, Loss: 1.4867\n",
      "Epoch: 67, Iteration: 101, Loss: 1.4995\n",
      "Epoch: 67, Iteration: 151, Loss: 1.5079\n",
      "Epoch: 67, Iteration: 201, Loss: 1.4951\n",
      "Epoch: 67, Iteration: 251, Loss: 1.5082\n",
      "Epoch: 67, Iteration: 301, Loss: 1.4752\n",
      "Epoch: 67, Iteration: 351, Loss: 1.4968\n",
      "Epoch: 68, Iteration: 1, Loss: 1.4800\n",
      "Epoch: 68, Iteration: 51, Loss: 1.5131\n",
      "Epoch: 68, Iteration: 101, Loss: 1.5001\n",
      "Epoch: 68, Iteration: 151, Loss: 1.5329\n",
      "Epoch: 68, Iteration: 201, Loss: 1.4921\n",
      "Epoch: 68, Iteration: 251, Loss: 1.5004\n",
      "Epoch: 68, Iteration: 301, Loss: 1.5064\n",
      "Epoch: 68, Iteration: 351, Loss: 1.5047\n",
      "Epoch: 69, Iteration: 1, Loss: 1.4994\n",
      "Epoch: 69, Iteration: 51, Loss: 1.5002\n",
      "Epoch: 69, Iteration: 101, Loss: 1.5232\n",
      "Epoch: 69, Iteration: 151, Loss: 1.5080\n",
      "Epoch: 69, Iteration: 201, Loss: 1.5129\n",
      "Epoch: 69, Iteration: 251, Loss: 1.5068\n",
      "Epoch: 69, Iteration: 301, Loss: 1.4852\n",
      "Epoch: 69, Iteration: 351, Loss: 1.4783\n",
      "Epoch: 70, Iteration: 1, Loss: 1.5123\n",
      "Epoch: 70, Iteration: 51, Loss: 1.5316\n",
      "Epoch: 70, Iteration: 101, Loss: 1.4772\n",
      "Epoch: 70, Iteration: 151, Loss: 1.5138\n",
      "Epoch: 70, Iteration: 201, Loss: 1.4895\n",
      "Epoch: 70, Iteration: 251, Loss: 1.4901\n",
      "Epoch: 70, Iteration: 301, Loss: 1.5014\n",
      "Epoch: 70, Iteration: 351, Loss: 1.4849\n",
      "Epoch: 71, Iteration: 1, Loss: 1.5241\n",
      "Epoch: 71, Iteration: 51, Loss: 1.4788\n",
      "Epoch: 71, Iteration: 101, Loss: 1.5153\n",
      "Epoch: 71, Iteration: 151, Loss: 1.5009\n",
      "Epoch: 71, Iteration: 201, Loss: 1.4771\n",
      "Epoch: 71, Iteration: 251, Loss: 1.5196\n",
      "Epoch: 71, Iteration: 301, Loss: 1.4946\n",
      "Epoch: 71, Iteration: 351, Loss: 1.4928\n",
      "Epoch: 72, Iteration: 1, Loss: 1.5070\n",
      "Epoch: 72, Iteration: 51, Loss: 1.4872\n",
      "Epoch: 72, Iteration: 101, Loss: 1.5312\n",
      "Epoch: 72, Iteration: 151, Loss: 1.5000\n",
      "Epoch: 72, Iteration: 201, Loss: 1.4922\n",
      "Epoch: 72, Iteration: 251, Loss: 1.4696\n",
      "Epoch: 72, Iteration: 301, Loss: 1.4926\n",
      "Epoch: 72, Iteration: 351, Loss: 1.4715\n",
      "Epoch: 73, Iteration: 1, Loss: 1.4863\n",
      "Epoch: 73, Iteration: 51, Loss: 1.4690\n",
      "Epoch: 73, Iteration: 101, Loss: 1.4928\n",
      "Epoch: 73, Iteration: 151, Loss: 1.5159\n",
      "Epoch: 73, Iteration: 201, Loss: 1.4722\n",
      "Epoch: 73, Iteration: 251, Loss: 1.4921\n",
      "Epoch: 73, Iteration: 301, Loss: 1.5236\n",
      "Epoch: 73, Iteration: 351, Loss: 1.5472\n",
      "Epoch: 74, Iteration: 1, Loss: 1.5088\n",
      "Epoch: 74, Iteration: 51, Loss: 1.5039\n",
      "Epoch: 74, Iteration: 101, Loss: 1.4919\n",
      "Epoch: 74, Iteration: 151, Loss: 1.5241\n",
      "Epoch: 74, Iteration: 201, Loss: 1.5168\n",
      "Epoch: 74, Iteration: 251, Loss: 1.5206\n",
      "Epoch: 74, Iteration: 301, Loss: 1.4825\n",
      "Epoch: 74, Iteration: 351, Loss: 1.4999\n",
      "Epoch: 75, Iteration: 1, Loss: 1.4976\n",
      "Epoch: 75, Iteration: 51, Loss: 1.5003\n",
      "Epoch: 75, Iteration: 101, Loss: 1.5158\n",
      "Epoch: 75, Iteration: 151, Loss: 1.4859\n",
      "Epoch: 75, Iteration: 201, Loss: 1.5087\n",
      "Epoch: 75, Iteration: 251, Loss: 1.4929\n",
      "Epoch: 75, Iteration: 301, Loss: 1.4889\n",
      "Epoch: 75, Iteration: 351, Loss: 1.5009\n",
      "Epoch: 76, Iteration: 1, Loss: 1.4988\n",
      "Epoch: 76, Iteration: 51, Loss: 1.4845\n",
      "Epoch: 76, Iteration: 101, Loss: 1.4926\n",
      "Epoch: 76, Iteration: 151, Loss: 1.4848\n",
      "Epoch: 76, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 76, Iteration: 251, Loss: 1.5001\n",
      "Epoch: 76, Iteration: 301, Loss: 1.4769\n",
      "Epoch: 76, Iteration: 351, Loss: 1.5141\n",
      "Epoch: 77, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 77, Iteration: 51, Loss: 1.5098\n",
      "Epoch: 77, Iteration: 101, Loss: 1.4776\n",
      "Epoch: 77, Iteration: 151, Loss: 1.5072\n",
      "Epoch: 77, Iteration: 201, Loss: 1.5017\n",
      "Epoch: 77, Iteration: 251, Loss: 1.5005\n",
      "Epoch: 77, Iteration: 301, Loss: 1.5463\n",
      "Epoch: 77, Iteration: 351, Loss: 1.4899\n",
      "Epoch: 78, Iteration: 1, Loss: 1.5348\n",
      "Epoch: 78, Iteration: 51, Loss: 1.4774\n",
      "Epoch: 78, Iteration: 101, Loss: 1.5161\n",
      "Epoch: 78, Iteration: 151, Loss: 1.5080\n",
      "Epoch: 78, Iteration: 201, Loss: 1.4929\n",
      "Epoch: 78, Iteration: 251, Loss: 1.4773\n",
      "Epoch: 78, Iteration: 301, Loss: 1.4937\n",
      "Epoch: 78, Iteration: 351, Loss: 1.4838\n",
      "Epoch: 79, Iteration: 1, Loss: 1.4847\n",
      "Epoch: 79, Iteration: 51, Loss: 1.4844\n",
      "Epoch: 79, Iteration: 101, Loss: 1.5072\n",
      "Epoch: 79, Iteration: 151, Loss: 1.4855\n",
      "Epoch: 79, Iteration: 201, Loss: 1.5029\n",
      "Epoch: 79, Iteration: 251, Loss: 1.5000\n",
      "Epoch: 79, Iteration: 301, Loss: 1.4904\n",
      "Epoch: 79, Iteration: 351, Loss: 1.4868\n",
      "Epoch: 80, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 80, Iteration: 51, Loss: 1.5152\n",
      "Epoch: 80, Iteration: 101, Loss: 1.4941\n",
      "Epoch: 80, Iteration: 151, Loss: 1.5102\n",
      "Epoch: 80, Iteration: 201, Loss: 1.4924\n",
      "Epoch: 80, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 80, Iteration: 301, Loss: 1.5340\n",
      "Epoch: 80, Iteration: 351, Loss: 1.5082\n",
      "Epoch: 81, Iteration: 1, Loss: 1.4926\n",
      "Epoch: 81, Iteration: 51, Loss: 1.4783\n",
      "Epoch: 81, Iteration: 101, Loss: 1.4928\n",
      "Epoch: 81, Iteration: 151, Loss: 1.4997\n",
      "Epoch: 81, Iteration: 201, Loss: 1.5231\n",
      "Epoch: 81, Iteration: 251, Loss: 1.4977\n",
      "Epoch: 81, Iteration: 301, Loss: 1.4843\n",
      "Epoch: 81, Iteration: 351, Loss: 1.4932\n",
      "Epoch: 82, Iteration: 1, Loss: 1.4847\n",
      "Epoch: 82, Iteration: 51, Loss: 1.4923\n",
      "Epoch: 82, Iteration: 101, Loss: 1.5413\n",
      "Epoch: 82, Iteration: 151, Loss: 1.4843\n",
      "Epoch: 82, Iteration: 201, Loss: 1.4848\n",
      "Epoch: 82, Iteration: 251, Loss: 1.5132\n",
      "Epoch: 82, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 82, Iteration: 351, Loss: 1.5080\n",
      "Epoch: 83, Iteration: 1, Loss: 1.4924\n",
      "Epoch: 83, Iteration: 51, Loss: 1.4925\n",
      "Epoch: 83, Iteration: 101, Loss: 1.4794\n",
      "Epoch: 83, Iteration: 151, Loss: 1.5238\n",
      "Epoch: 83, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 83, Iteration: 251, Loss: 1.5060\n",
      "Epoch: 83, Iteration: 301, Loss: 1.4921\n",
      "Epoch: 83, Iteration: 351, Loss: 1.5038\n",
      "Epoch: 84, Iteration: 1, Loss: 1.5154\n",
      "Epoch: 84, Iteration: 51, Loss: 1.4771\n",
      "Epoch: 84, Iteration: 101, Loss: 1.4844\n",
      "Epoch: 84, Iteration: 151, Loss: 1.5103\n",
      "Epoch: 84, Iteration: 201, Loss: 1.5001\n",
      "Epoch: 84, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 84, Iteration: 301, Loss: 1.5069\n",
      "Epoch: 84, Iteration: 351, Loss: 1.4999\n",
      "Epoch: 85, Iteration: 1, Loss: 1.5159\n",
      "Epoch: 85, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 85, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 85, Iteration: 151, Loss: 1.5164\n",
      "Epoch: 85, Iteration: 201, Loss: 1.5113\n",
      "Epoch: 85, Iteration: 251, Loss: 1.4688\n",
      "Epoch: 85, Iteration: 301, Loss: 1.4925\n",
      "Epoch: 85, Iteration: 351, Loss: 1.5168\n",
      "Epoch: 86, Iteration: 1, Loss: 1.4776\n",
      "Epoch: 86, Iteration: 51, Loss: 1.4846\n",
      "Epoch: 86, Iteration: 101, Loss: 1.5078\n",
      "Epoch: 86, Iteration: 151, Loss: 1.5028\n",
      "Epoch: 86, Iteration: 201, Loss: 1.4854\n",
      "Epoch: 86, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 86, Iteration: 301, Loss: 1.4725\n",
      "Epoch: 86, Iteration: 351, Loss: 1.4764\n",
      "Epoch: 87, Iteration: 1, Loss: 1.5073\n",
      "Epoch: 87, Iteration: 51, Loss: 1.4829\n",
      "Epoch: 87, Iteration: 101, Loss: 1.5080\n",
      "Epoch: 87, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 87, Iteration: 201, Loss: 1.4910\n",
      "Epoch: 87, Iteration: 251, Loss: 1.5161\n",
      "Epoch: 87, Iteration: 301, Loss: 1.4612\n",
      "Epoch: 87, Iteration: 351, Loss: 1.5010\n",
      "Epoch: 88, Iteration: 1, Loss: 1.5005\n",
      "Epoch: 88, Iteration: 51, Loss: 1.5186\n",
      "Epoch: 88, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 88, Iteration: 151, Loss: 1.4924\n",
      "Epoch: 88, Iteration: 201, Loss: 1.4971\n",
      "Epoch: 88, Iteration: 251, Loss: 1.4959\n",
      "Epoch: 88, Iteration: 301, Loss: 1.4849\n",
      "Epoch: 88, Iteration: 351, Loss: 1.5227\n",
      "Epoch: 89, Iteration: 1, Loss: 1.4921\n",
      "Epoch: 89, Iteration: 51, Loss: 1.4845\n",
      "Epoch: 89, Iteration: 101, Loss: 1.4692\n",
      "Epoch: 89, Iteration: 151, Loss: 1.5258\n",
      "Epoch: 89, Iteration: 201, Loss: 1.5201\n",
      "Epoch: 89, Iteration: 251, Loss: 1.4716\n",
      "Epoch: 89, Iteration: 301, Loss: 1.5001\n",
      "Epoch: 89, Iteration: 351, Loss: 1.4768\n",
      "Epoch: 90, Iteration: 1, Loss: 1.5089\n",
      "Epoch: 90, Iteration: 51, Loss: 1.4945\n",
      "Epoch: 90, Iteration: 101, Loss: 1.5161\n",
      "Epoch: 90, Iteration: 151, Loss: 1.5155\n",
      "Epoch: 90, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 90, Iteration: 251, Loss: 1.4892\n",
      "Epoch: 90, Iteration: 301, Loss: 1.5156\n",
      "Epoch: 90, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 91, Iteration: 1, Loss: 1.4856\n",
      "Epoch: 91, Iteration: 51, Loss: 1.5119\n",
      "Epoch: 91, Iteration: 101, Loss: 1.4844\n",
      "Epoch: 91, Iteration: 151, Loss: 1.4769\n",
      "Epoch: 91, Iteration: 201, Loss: 1.4613\n",
      "Epoch: 91, Iteration: 251, Loss: 1.4692\n",
      "Epoch: 91, Iteration: 301, Loss: 1.4755\n",
      "Epoch: 91, Iteration: 351, Loss: 1.5234\n",
      "Epoch: 92, Iteration: 1, Loss: 1.4875\n",
      "Epoch: 92, Iteration: 51, Loss: 1.4861\n",
      "Epoch: 92, Iteration: 101, Loss: 1.5080\n",
      "Epoch: 92, Iteration: 151, Loss: 1.4923\n",
      "Epoch: 92, Iteration: 201, Loss: 1.4815\n",
      "Epoch: 92, Iteration: 251, Loss: 1.5080\n",
      "Epoch: 92, Iteration: 301, Loss: 1.4978\n",
      "Epoch: 92, Iteration: 351, Loss: 1.4752\n",
      "Epoch: 93, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 93, Iteration: 51, Loss: 1.5091\n",
      "Epoch: 93, Iteration: 101, Loss: 1.4845\n",
      "Epoch: 93, Iteration: 151, Loss: 1.4872\n",
      "Epoch: 93, Iteration: 201, Loss: 1.5025\n",
      "Epoch: 93, Iteration: 251, Loss: 1.5156\n",
      "Epoch: 93, Iteration: 301, Loss: 1.4612\n",
      "Epoch: 93, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 94, Iteration: 1, Loss: 1.5080\n",
      "Epoch: 94, Iteration: 51, Loss: 1.4932\n",
      "Epoch: 94, Iteration: 101, Loss: 1.5081\n",
      "Epoch: 94, Iteration: 151, Loss: 1.5002\n",
      "Epoch: 94, Iteration: 201, Loss: 1.4999\n",
      "Epoch: 94, Iteration: 251, Loss: 1.5000\n",
      "Epoch: 94, Iteration: 301, Loss: 1.4770\n",
      "Epoch: 94, Iteration: 351, Loss: 1.5316\n",
      "Epoch: 95, Iteration: 1, Loss: 1.5158\n",
      "Epoch: 95, Iteration: 51, Loss: 1.4893\n",
      "Epoch: 95, Iteration: 101, Loss: 1.4841\n",
      "Epoch: 95, Iteration: 151, Loss: 1.5000\n",
      "Epoch: 95, Iteration: 201, Loss: 1.4843\n",
      "Epoch: 95, Iteration: 251, Loss: 1.4917\n",
      "Epoch: 95, Iteration: 301, Loss: 1.4792\n",
      "Epoch: 95, Iteration: 351, Loss: 1.5002\n",
      "Epoch: 96, Iteration: 1, Loss: 1.5002\n",
      "Epoch: 96, Iteration: 51, Loss: 1.4894\n",
      "Epoch: 96, Iteration: 101, Loss: 1.4890\n",
      "Epoch: 96, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 96, Iteration: 201, Loss: 1.4850\n",
      "Epoch: 96, Iteration: 251, Loss: 1.5019\n",
      "Epoch: 96, Iteration: 301, Loss: 1.4925\n",
      "Epoch: 96, Iteration: 351, Loss: 1.4612\n",
      "Epoch: 97, Iteration: 1, Loss: 1.5005\n",
      "Epoch: 97, Iteration: 51, Loss: 1.5237\n",
      "Epoch: 97, Iteration: 101, Loss: 1.5084\n",
      "Epoch: 97, Iteration: 151, Loss: 1.5002\n",
      "Epoch: 97, Iteration: 201, Loss: 1.4690\n",
      "Epoch: 97, Iteration: 251, Loss: 1.5002\n",
      "Epoch: 97, Iteration: 301, Loss: 1.5161\n",
      "Epoch: 97, Iteration: 351, Loss: 1.5084\n",
      "Epoch: 98, Iteration: 1, Loss: 1.4913\n",
      "Epoch: 98, Iteration: 51, Loss: 1.4845\n",
      "Epoch: 98, Iteration: 101, Loss: 1.4847\n",
      "Epoch: 98, Iteration: 151, Loss: 1.5091\n",
      "Epoch: 98, Iteration: 201, Loss: 1.4850\n",
      "Epoch: 98, Iteration: 251, Loss: 1.4910\n",
      "Epoch: 98, Iteration: 301, Loss: 1.4921\n",
      "Epoch: 98, Iteration: 351, Loss: 1.4966\n",
      "Epoch: 99, Iteration: 1, Loss: 1.4782\n",
      "Epoch: 99, Iteration: 51, Loss: 1.4999\n",
      "Epoch: 99, Iteration: 101, Loss: 1.4847\n",
      "Epoch: 99, Iteration: 151, Loss: 1.4997\n",
      "Epoch: 99, Iteration: 201, Loss: 1.5004\n",
      "Epoch: 99, Iteration: 251, Loss: 1.5002\n",
      "Epoch: 99, Iteration: 301, Loss: 1.4870\n",
      "Epoch: 99, Iteration: 351, Loss: 1.5085\n",
      "Epoch: 100, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 100, Iteration: 51, Loss: 1.4612\n",
      "Epoch: 100, Iteration: 101, Loss: 1.4946\n",
      "Epoch: 100, Iteration: 151, Loss: 1.5009\n",
      "Epoch: 100, Iteration: 201, Loss: 1.4988\n",
      "Epoch: 100, Iteration: 251, Loss: 1.5110\n",
      "Epoch: 100, Iteration: 301, Loss: 1.4933\n",
      "Epoch: 100, Iteration: 351, Loss: 1.4958\n",
      "Epoch: 101, Iteration: 1, Loss: 1.4929\n",
      "Epoch: 101, Iteration: 51, Loss: 1.4922\n",
      "Epoch: 101, Iteration: 101, Loss: 1.4923\n",
      "Epoch: 101, Iteration: 151, Loss: 1.4848\n",
      "Epoch: 101, Iteration: 201, Loss: 1.5181\n",
      "Epoch: 101, Iteration: 251, Loss: 1.4999\n",
      "Epoch: 101, Iteration: 301, Loss: 1.5003\n",
      "Epoch: 101, Iteration: 351, Loss: 1.4866\n",
      "Epoch: 102, Iteration: 1, Loss: 1.4926\n",
      "Epoch: 102, Iteration: 51, Loss: 1.5151\n",
      "Epoch: 102, Iteration: 101, Loss: 1.4690\n",
      "Epoch: 102, Iteration: 151, Loss: 1.4850\n",
      "Epoch: 102, Iteration: 201, Loss: 1.5070\n",
      "Epoch: 102, Iteration: 251, Loss: 1.4930\n",
      "Epoch: 102, Iteration: 301, Loss: 1.5235\n",
      "Epoch: 102, Iteration: 351, Loss: 1.4783\n",
      "Epoch: 103, Iteration: 1, Loss: 1.4613\n",
      "Epoch: 103, Iteration: 51, Loss: 1.4848\n",
      "Epoch: 103, Iteration: 101, Loss: 1.4845\n",
      "Epoch: 103, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 103, Iteration: 201, Loss: 1.4932\n",
      "Epoch: 103, Iteration: 251, Loss: 1.4777\n",
      "Epoch: 103, Iteration: 301, Loss: 1.4769\n",
      "Epoch: 103, Iteration: 351, Loss: 1.4692\n",
      "Epoch: 104, Iteration: 1, Loss: 1.4845\n",
      "Epoch: 104, Iteration: 51, Loss: 1.4772\n",
      "Epoch: 104, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 104, Iteration: 151, Loss: 1.5001\n",
      "Epoch: 104, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 104, Iteration: 251, Loss: 1.5081\n",
      "Epoch: 104, Iteration: 301, Loss: 1.5006\n",
      "Epoch: 104, Iteration: 351, Loss: 1.4925\n",
      "Epoch: 105, Iteration: 1, Loss: 1.5003\n",
      "Epoch: 105, Iteration: 51, Loss: 1.4766\n",
      "Epoch: 105, Iteration: 101, Loss: 1.4842\n",
      "Epoch: 105, Iteration: 151, Loss: 1.5156\n",
      "Epoch: 105, Iteration: 201, Loss: 1.4845\n",
      "Epoch: 105, Iteration: 251, Loss: 1.5041\n",
      "Epoch: 105, Iteration: 301, Loss: 1.4925\n",
      "Epoch: 105, Iteration: 351, Loss: 1.5122\n",
      "Epoch: 106, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 106, Iteration: 51, Loss: 1.4765\n",
      "Epoch: 106, Iteration: 101, Loss: 1.4921\n",
      "Epoch: 106, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 106, Iteration: 201, Loss: 1.4935\n",
      "Epoch: 106, Iteration: 251, Loss: 1.4968\n",
      "Epoch: 106, Iteration: 301, Loss: 1.4996\n",
      "Epoch: 106, Iteration: 351, Loss: 1.4612\n",
      "Epoch: 107, Iteration: 1, Loss: 1.4765\n",
      "Epoch: 107, Iteration: 51, Loss: 1.4835\n",
      "Epoch: 107, Iteration: 101, Loss: 1.5001\n",
      "Epoch: 107, Iteration: 151, Loss: 1.5172\n",
      "Epoch: 107, Iteration: 201, Loss: 1.5001\n",
      "Epoch: 107, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 107, Iteration: 301, Loss: 1.5077\n",
      "Epoch: 107, Iteration: 351, Loss: 1.4774\n",
      "Epoch: 108, Iteration: 1, Loss: 1.4924\n",
      "Epoch: 108, Iteration: 51, Loss: 1.4941\n",
      "Epoch: 108, Iteration: 101, Loss: 1.5009\n",
      "Epoch: 108, Iteration: 151, Loss: 1.4797\n",
      "Epoch: 108, Iteration: 201, Loss: 1.4782\n",
      "Epoch: 108, Iteration: 251, Loss: 1.4730\n",
      "Epoch: 108, Iteration: 301, Loss: 1.5233\n",
      "Epoch: 108, Iteration: 351, Loss: 1.5021\n",
      "Epoch: 109, Iteration: 1, Loss: 1.4921\n",
      "Epoch: 109, Iteration: 51, Loss: 1.5157\n",
      "Epoch: 109, Iteration: 101, Loss: 1.5324\n",
      "Epoch: 109, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 109, Iteration: 201, Loss: 1.4690\n",
      "Epoch: 109, Iteration: 251, Loss: 1.4689\n",
      "Epoch: 109, Iteration: 301, Loss: 1.4999\n",
      "Epoch: 109, Iteration: 351, Loss: 1.5106\n",
      "Epoch: 110, Iteration: 1, Loss: 1.4847\n",
      "Epoch: 110, Iteration: 51, Loss: 1.4929\n",
      "Epoch: 110, Iteration: 101, Loss: 1.4844\n",
      "Epoch: 110, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 110, Iteration: 201, Loss: 1.5078\n",
      "Epoch: 110, Iteration: 251, Loss: 1.4928\n",
      "Epoch: 110, Iteration: 301, Loss: 1.4722\n",
      "Epoch: 110, Iteration: 351, Loss: 1.4987\n",
      "Epoch: 111, Iteration: 1, Loss: 1.5002\n",
      "Epoch: 111, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 111, Iteration: 101, Loss: 1.4690\n",
      "Epoch: 111, Iteration: 151, Loss: 1.4845\n",
      "Epoch: 111, Iteration: 201, Loss: 1.5078\n",
      "Epoch: 111, Iteration: 251, Loss: 1.4690\n",
      "Epoch: 111, Iteration: 301, Loss: 1.4612\n",
      "Epoch: 111, Iteration: 351, Loss: 1.4768\n",
      "Epoch: 112, Iteration: 1, Loss: 1.4971\n",
      "Epoch: 112, Iteration: 51, Loss: 1.5078\n",
      "Epoch: 112, Iteration: 101, Loss: 1.4694\n",
      "Epoch: 112, Iteration: 151, Loss: 1.4922\n",
      "Epoch: 112, Iteration: 201, Loss: 1.5038\n",
      "Epoch: 112, Iteration: 251, Loss: 1.4843\n",
      "Epoch: 112, Iteration: 301, Loss: 1.4846\n",
      "Epoch: 112, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 113, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 113, Iteration: 51, Loss: 1.5081\n",
      "Epoch: 113, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 113, Iteration: 151, Loss: 1.5001\n",
      "Epoch: 113, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 113, Iteration: 251, Loss: 1.4689\n",
      "Epoch: 113, Iteration: 301, Loss: 1.4978\n",
      "Epoch: 113, Iteration: 351, Loss: 1.4867\n",
      "Epoch: 114, Iteration: 1, Loss: 1.4772\n",
      "Epoch: 114, Iteration: 51, Loss: 1.5004\n",
      "Epoch: 114, Iteration: 101, Loss: 1.5002\n",
      "Epoch: 114, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 114, Iteration: 201, Loss: 1.4927\n",
      "Epoch: 114, Iteration: 251, Loss: 1.4921\n",
      "Epoch: 114, Iteration: 301, Loss: 1.4844\n",
      "Epoch: 114, Iteration: 351, Loss: 1.4915\n",
      "Epoch: 115, Iteration: 1, Loss: 1.4923\n",
      "Epoch: 115, Iteration: 51, Loss: 1.5188\n",
      "Epoch: 115, Iteration: 101, Loss: 1.4924\n",
      "Epoch: 115, Iteration: 151, Loss: 1.4773\n",
      "Epoch: 115, Iteration: 201, Loss: 1.4690\n",
      "Epoch: 115, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 115, Iteration: 301, Loss: 1.4690\n",
      "Epoch: 115, Iteration: 351, Loss: 1.4612\n",
      "Epoch: 116, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 116, Iteration: 51, Loss: 1.4846\n",
      "Epoch: 116, Iteration: 101, Loss: 1.4765\n",
      "Epoch: 116, Iteration: 151, Loss: 1.5144\n",
      "Epoch: 116, Iteration: 201, Loss: 1.4774\n",
      "Epoch: 116, Iteration: 251, Loss: 1.4937\n",
      "Epoch: 116, Iteration: 301, Loss: 1.4845\n",
      "Epoch: 116, Iteration: 351, Loss: 1.4768\n",
      "Epoch: 117, Iteration: 1, Loss: 1.5080\n",
      "Epoch: 117, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 117, Iteration: 101, Loss: 1.5002\n",
      "Epoch: 117, Iteration: 151, Loss: 1.5241\n",
      "Epoch: 117, Iteration: 201, Loss: 1.4914\n",
      "Epoch: 117, Iteration: 251, Loss: 1.5002\n",
      "Epoch: 117, Iteration: 301, Loss: 1.5077\n",
      "Epoch: 117, Iteration: 351, Loss: 1.5236\n",
      "Epoch: 118, Iteration: 1, Loss: 1.4932\n",
      "Epoch: 118, Iteration: 51, Loss: 1.4921\n",
      "Epoch: 118, Iteration: 101, Loss: 1.4784\n",
      "Epoch: 118, Iteration: 151, Loss: 1.5158\n",
      "Epoch: 118, Iteration: 201, Loss: 1.4689\n",
      "Epoch: 118, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 118, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 118, Iteration: 351, Loss: 1.4845\n",
      "Epoch: 119, Iteration: 1, Loss: 1.5010\n",
      "Epoch: 119, Iteration: 51, Loss: 1.4772\n",
      "Epoch: 119, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 119, Iteration: 151, Loss: 1.4769\n",
      "Epoch: 119, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 119, Iteration: 251, Loss: 1.4890\n",
      "Epoch: 119, Iteration: 301, Loss: 1.5081\n",
      "Epoch: 119, Iteration: 351, Loss: 1.4928\n",
      "Epoch: 120, Iteration: 1, Loss: 1.4924\n",
      "Epoch: 120, Iteration: 51, Loss: 1.5003\n",
      "Epoch: 120, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 120, Iteration: 151, Loss: 1.5002\n",
      "Epoch: 120, Iteration: 201, Loss: 1.4845\n",
      "Epoch: 120, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 120, Iteration: 301, Loss: 1.4765\n",
      "Epoch: 120, Iteration: 351, Loss: 1.5154\n",
      "Epoch: 121, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 121, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 121, Iteration: 101, Loss: 1.4687\n",
      "Epoch: 121, Iteration: 151, Loss: 1.4612\n",
      "Epoch: 121, Iteration: 201, Loss: 1.4845\n",
      "Epoch: 121, Iteration: 251, Loss: 1.5154\n",
      "Epoch: 121, Iteration: 301, Loss: 1.4925\n",
      "Epoch: 121, Iteration: 351, Loss: 1.4755\n",
      "Epoch: 122, Iteration: 1, Loss: 1.5113\n",
      "Epoch: 122, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 122, Iteration: 101, Loss: 1.4924\n",
      "Epoch: 122, Iteration: 151, Loss: 1.4922\n",
      "Epoch: 122, Iteration: 201, Loss: 1.4859\n",
      "Epoch: 122, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 122, Iteration: 301, Loss: 1.4925\n",
      "Epoch: 122, Iteration: 351, Loss: 1.4845\n",
      "Epoch: 123, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 123, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 123, Iteration: 101, Loss: 1.5003\n",
      "Epoch: 123, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 123, Iteration: 201, Loss: 1.4696\n",
      "Epoch: 123, Iteration: 251, Loss: 1.5077\n",
      "Epoch: 123, Iteration: 301, Loss: 1.4845\n",
      "Epoch: 123, Iteration: 351, Loss: 1.4777\n",
      "Epoch: 124, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 124, Iteration: 51, Loss: 1.5001\n",
      "Epoch: 124, Iteration: 101, Loss: 1.4843\n",
      "Epoch: 124, Iteration: 151, Loss: 1.5022\n",
      "Epoch: 124, Iteration: 201, Loss: 1.5076\n",
      "Epoch: 124, Iteration: 251, Loss: 1.5307\n",
      "Epoch: 124, Iteration: 301, Loss: 1.4922\n",
      "Epoch: 124, Iteration: 351, Loss: 1.4767\n",
      "Epoch: 125, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 125, Iteration: 51, Loss: 1.5005\n",
      "Epoch: 125, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 125, Iteration: 151, Loss: 1.4922\n",
      "Epoch: 125, Iteration: 201, Loss: 1.5005\n",
      "Epoch: 125, Iteration: 251, Loss: 1.5156\n",
      "Epoch: 125, Iteration: 301, Loss: 1.4769\n",
      "Epoch: 125, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 126, Iteration: 1, Loss: 1.5162\n",
      "Epoch: 126, Iteration: 51, Loss: 1.4846\n",
      "Epoch: 126, Iteration: 101, Loss: 1.4727\n",
      "Epoch: 126, Iteration: 151, Loss: 1.5000\n",
      "Epoch: 126, Iteration: 201, Loss: 1.5079\n",
      "Epoch: 126, Iteration: 251, Loss: 1.4687\n",
      "Epoch: 126, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 126, Iteration: 351, Loss: 1.4923\n",
      "Epoch: 127, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 127, Iteration: 51, Loss: 1.4922\n",
      "Epoch: 127, Iteration: 101, Loss: 1.4797\n",
      "Epoch: 127, Iteration: 151, Loss: 1.4690\n",
      "Epoch: 127, Iteration: 201, Loss: 1.4848\n",
      "Epoch: 127, Iteration: 251, Loss: 1.5010\n",
      "Epoch: 127, Iteration: 301, Loss: 1.5077\n",
      "Epoch: 127, Iteration: 351, Loss: 1.4952\n",
      "Epoch: 128, Iteration: 1, Loss: 1.5010\n",
      "Epoch: 128, Iteration: 51, Loss: 1.4687\n",
      "Epoch: 128, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 128, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 128, Iteration: 201, Loss: 1.4922\n",
      "Epoch: 128, Iteration: 251, Loss: 1.4825\n",
      "Epoch: 128, Iteration: 301, Loss: 1.4767\n",
      "Epoch: 128, Iteration: 351, Loss: 1.4612\n",
      "Epoch: 129, Iteration: 1, Loss: 1.4862\n",
      "Epoch: 129, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 129, Iteration: 101, Loss: 1.5002\n",
      "Epoch: 129, Iteration: 151, Loss: 1.5081\n",
      "Epoch: 129, Iteration: 201, Loss: 1.5090\n",
      "Epoch: 129, Iteration: 251, Loss: 1.5177\n",
      "Epoch: 129, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 129, Iteration: 351, Loss: 1.5159\n",
      "Epoch: 130, Iteration: 1, Loss: 1.4999\n",
      "Epoch: 130, Iteration: 51, Loss: 1.5074\n",
      "Epoch: 130, Iteration: 101, Loss: 1.4690\n",
      "Epoch: 130, Iteration: 151, Loss: 1.4941\n",
      "Epoch: 130, Iteration: 201, Loss: 1.5063\n",
      "Epoch: 130, Iteration: 251, Loss: 1.4844\n",
      "Epoch: 130, Iteration: 301, Loss: 1.4612\n",
      "Epoch: 130, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 131, Iteration: 1, Loss: 1.5104\n",
      "Epoch: 131, Iteration: 51, Loss: 1.4727\n",
      "Epoch: 131, Iteration: 101, Loss: 1.5227\n",
      "Epoch: 131, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 131, Iteration: 201, Loss: 1.5160\n",
      "Epoch: 131, Iteration: 251, Loss: 1.4692\n",
      "Epoch: 131, Iteration: 301, Loss: 1.4617\n",
      "Epoch: 131, Iteration: 351, Loss: 1.4803\n",
      "Epoch: 132, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 132, Iteration: 51, Loss: 1.4612\n",
      "Epoch: 132, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 132, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 132, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 132, Iteration: 251, Loss: 1.4690\n",
      "Epoch: 132, Iteration: 301, Loss: 1.5003\n",
      "Epoch: 132, Iteration: 351, Loss: 1.4770\n",
      "Epoch: 133, Iteration: 1, Loss: 1.4891\n",
      "Epoch: 133, Iteration: 51, Loss: 1.4785\n",
      "Epoch: 133, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 133, Iteration: 151, Loss: 1.4844\n",
      "Epoch: 133, Iteration: 201, Loss: 1.4844\n",
      "Epoch: 133, Iteration: 251, Loss: 1.4705\n",
      "Epoch: 133, Iteration: 301, Loss: 1.4844\n",
      "Epoch: 133, Iteration: 351, Loss: 1.5077\n",
      "Epoch: 134, Iteration: 1, Loss: 1.4923\n",
      "Epoch: 134, Iteration: 51, Loss: 1.5003\n",
      "Epoch: 134, Iteration: 101, Loss: 1.4920\n",
      "Epoch: 134, Iteration: 151, Loss: 1.4767\n",
      "Epoch: 134, Iteration: 201, Loss: 1.4631\n",
      "Epoch: 134, Iteration: 251, Loss: 1.4856\n",
      "Epoch: 134, Iteration: 301, Loss: 1.4857\n",
      "Epoch: 134, Iteration: 351, Loss: 1.4921\n",
      "Epoch: 135, Iteration: 1, Loss: 1.4847\n",
      "Epoch: 135, Iteration: 51, Loss: 1.5156\n",
      "Epoch: 135, Iteration: 101, Loss: 1.4869\n",
      "Epoch: 135, Iteration: 151, Loss: 1.4995\n",
      "Epoch: 135, Iteration: 201, Loss: 1.4764\n",
      "Epoch: 135, Iteration: 251, Loss: 1.4690\n",
      "Epoch: 135, Iteration: 301, Loss: 1.4766\n",
      "Epoch: 135, Iteration: 351, Loss: 1.4924\n",
      "Epoch: 136, Iteration: 1, Loss: 1.5156\n",
      "Epoch: 136, Iteration: 51, Loss: 1.4690\n",
      "Epoch: 136, Iteration: 101, Loss: 1.5154\n",
      "Epoch: 136, Iteration: 151, Loss: 1.4921\n",
      "Epoch: 136, Iteration: 201, Loss: 1.5043\n",
      "Epoch: 136, Iteration: 251, Loss: 1.4958\n",
      "Epoch: 136, Iteration: 301, Loss: 1.5002\n",
      "Epoch: 136, Iteration: 351, Loss: 1.5002\n",
      "Epoch: 137, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 137, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 137, Iteration: 101, Loss: 1.4924\n",
      "Epoch: 137, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 137, Iteration: 201, Loss: 1.4912\n",
      "Epoch: 137, Iteration: 251, Loss: 1.4767\n",
      "Epoch: 137, Iteration: 301, Loss: 1.4972\n",
      "Epoch: 137, Iteration: 351, Loss: 1.5075\n",
      "Epoch: 138, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 138, Iteration: 51, Loss: 1.4997\n",
      "Epoch: 138, Iteration: 101, Loss: 1.4687\n",
      "Epoch: 138, Iteration: 151, Loss: 1.4845\n",
      "Epoch: 138, Iteration: 201, Loss: 1.4922\n",
      "Epoch: 138, Iteration: 251, Loss: 1.4923\n",
      "Epoch: 138, Iteration: 301, Loss: 1.4922\n",
      "Epoch: 138, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 139, Iteration: 1, Loss: 1.5001\n",
      "Epoch: 139, Iteration: 51, Loss: 1.5089\n",
      "Epoch: 139, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 139, Iteration: 151, Loss: 1.5150\n",
      "Epoch: 139, Iteration: 201, Loss: 1.4690\n",
      "Epoch: 139, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 139, Iteration: 301, Loss: 1.4922\n",
      "Epoch: 139, Iteration: 351, Loss: 1.4861\n",
      "Epoch: 140, Iteration: 1, Loss: 1.4923\n",
      "Epoch: 140, Iteration: 51, Loss: 1.4769\n",
      "Epoch: 140, Iteration: 101, Loss: 1.4924\n",
      "Epoch: 140, Iteration: 151, Loss: 1.5061\n",
      "Epoch: 140, Iteration: 201, Loss: 1.4765\n",
      "Epoch: 140, Iteration: 251, Loss: 1.4922\n",
      "Epoch: 140, Iteration: 301, Loss: 1.4690\n",
      "Epoch: 140, Iteration: 351, Loss: 1.4936\n",
      "Epoch: 141, Iteration: 1, Loss: 1.4921\n",
      "Epoch: 141, Iteration: 51, Loss: 1.4843\n",
      "Epoch: 141, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 141, Iteration: 151, Loss: 1.4845\n",
      "Epoch: 141, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 141, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 141, Iteration: 301, Loss: 1.4847\n",
      "Epoch: 141, Iteration: 351, Loss: 1.4691\n",
      "Epoch: 142, Iteration: 1, Loss: 1.4772\n",
      "Epoch: 142, Iteration: 51, Loss: 1.4692\n",
      "Epoch: 142, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 142, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 142, Iteration: 201, Loss: 1.5077\n",
      "Epoch: 142, Iteration: 251, Loss: 1.4864\n",
      "Epoch: 142, Iteration: 301, Loss: 1.4923\n",
      "Epoch: 142, Iteration: 351, Loss: 1.4879\n",
      "Epoch: 143, Iteration: 1, Loss: 1.4919\n",
      "Epoch: 143, Iteration: 51, Loss: 1.4767\n",
      "Epoch: 143, Iteration: 101, Loss: 1.4845\n",
      "Epoch: 143, Iteration: 151, Loss: 1.5316\n",
      "Epoch: 143, Iteration: 201, Loss: 1.5003\n",
      "Epoch: 143, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 143, Iteration: 301, Loss: 1.4776\n",
      "Epoch: 143, Iteration: 351, Loss: 1.4920\n",
      "Epoch: 144, Iteration: 1, Loss: 1.4843\n",
      "Epoch: 144, Iteration: 51, Loss: 1.4841\n",
      "Epoch: 144, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 144, Iteration: 151, Loss: 1.4617\n",
      "Epoch: 144, Iteration: 201, Loss: 1.5160\n",
      "Epoch: 144, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 144, Iteration: 301, Loss: 1.4765\n",
      "Epoch: 144, Iteration: 351, Loss: 1.5080\n",
      "Epoch: 145, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 145, Iteration: 51, Loss: 1.5079\n",
      "Epoch: 145, Iteration: 101, Loss: 1.4854\n",
      "Epoch: 145, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 145, Iteration: 201, Loss: 1.4689\n",
      "Epoch: 145, Iteration: 251, Loss: 1.5084\n",
      "Epoch: 145, Iteration: 301, Loss: 1.4845\n",
      "Epoch: 145, Iteration: 351, Loss: 1.4768\n",
      "Epoch: 146, Iteration: 1, Loss: 1.4844\n",
      "Epoch: 146, Iteration: 51, Loss: 1.4765\n",
      "Epoch: 146, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 146, Iteration: 151, Loss: 1.4853\n",
      "Epoch: 146, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 146, Iteration: 251, Loss: 1.5156\n",
      "Epoch: 146, Iteration: 301, Loss: 1.5162\n",
      "Epoch: 146, Iteration: 351, Loss: 1.4768\n",
      "Epoch: 147, Iteration: 1, Loss: 1.4919\n",
      "Epoch: 147, Iteration: 51, Loss: 1.4769\n",
      "Epoch: 147, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 147, Iteration: 151, Loss: 1.5097\n",
      "Epoch: 147, Iteration: 201, Loss: 1.4845\n",
      "Epoch: 147, Iteration: 251, Loss: 1.4884\n",
      "Epoch: 147, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 147, Iteration: 351, Loss: 1.4630\n",
      "Epoch: 148, Iteration: 1, Loss: 1.4977\n",
      "Epoch: 148, Iteration: 51, Loss: 1.4848\n",
      "Epoch: 148, Iteration: 101, Loss: 1.4920\n",
      "Epoch: 148, Iteration: 151, Loss: 1.4924\n",
      "Epoch: 148, Iteration: 201, Loss: 1.4845\n",
      "Epoch: 148, Iteration: 251, Loss: 1.4769\n",
      "Epoch: 148, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 148, Iteration: 351, Loss: 1.4936\n",
      "Epoch: 149, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 149, Iteration: 51, Loss: 1.4925\n",
      "Epoch: 149, Iteration: 101, Loss: 1.4924\n",
      "Epoch: 149, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 149, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 149, Iteration: 251, Loss: 1.4923\n",
      "Epoch: 149, Iteration: 301, Loss: 1.4616\n",
      "Epoch: 149, Iteration: 351, Loss: 1.4612\n",
      "Epoch: 150, Iteration: 1, Loss: 1.4858\n",
      "Epoch: 150, Iteration: 51, Loss: 1.5082\n",
      "Epoch: 150, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 150, Iteration: 151, Loss: 1.4695\n",
      "Epoch: 150, Iteration: 201, Loss: 1.4924\n",
      "Epoch: 150, Iteration: 251, Loss: 1.5002\n",
      "Epoch: 150, Iteration: 301, Loss: 1.4922\n",
      "Epoch: 150, Iteration: 351, Loss: 1.5024\n",
      "Epoch: 151, Iteration: 1, Loss: 1.4843\n",
      "Epoch: 151, Iteration: 51, Loss: 1.4925\n",
      "Epoch: 151, Iteration: 101, Loss: 1.4845\n",
      "Epoch: 151, Iteration: 151, Loss: 1.5002\n",
      "Epoch: 151, Iteration: 201, Loss: 1.4956\n",
      "Epoch: 151, Iteration: 251, Loss: 1.4844\n",
      "Epoch: 151, Iteration: 301, Loss: 1.4773\n",
      "Epoch: 151, Iteration: 351, Loss: 1.5005\n",
      "Epoch: 152, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 152, Iteration: 51, Loss: 1.4767\n",
      "Epoch: 152, Iteration: 101, Loss: 1.5237\n",
      "Epoch: 152, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 152, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 152, Iteration: 251, Loss: 1.5002\n",
      "Epoch: 152, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 152, Iteration: 351, Loss: 1.4924\n",
      "Epoch: 153, Iteration: 1, Loss: 1.5158\n",
      "Epoch: 153, Iteration: 51, Loss: 1.4690\n",
      "Epoch: 153, Iteration: 101, Loss: 1.5080\n",
      "Epoch: 153, Iteration: 151, Loss: 1.4920\n",
      "Epoch: 153, Iteration: 201, Loss: 1.4767\n",
      "Epoch: 153, Iteration: 251, Loss: 1.4844\n",
      "Epoch: 153, Iteration: 301, Loss: 1.4846\n",
      "Epoch: 153, Iteration: 351, Loss: 1.4843\n",
      "Epoch: 154, Iteration: 1, Loss: 1.4919\n",
      "Epoch: 154, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 154, Iteration: 101, Loss: 1.4920\n",
      "Epoch: 154, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 154, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 154, Iteration: 251, Loss: 1.4767\n",
      "Epoch: 154, Iteration: 301, Loss: 1.5002\n",
      "Epoch: 154, Iteration: 351, Loss: 1.5074\n",
      "Epoch: 155, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 155, Iteration: 51, Loss: 1.4847\n",
      "Epoch: 155, Iteration: 101, Loss: 1.5078\n",
      "Epoch: 155, Iteration: 151, Loss: 1.5083\n",
      "Epoch: 155, Iteration: 201, Loss: 1.4922\n",
      "Epoch: 155, Iteration: 251, Loss: 1.4922\n",
      "Epoch: 155, Iteration: 301, Loss: 1.4774\n",
      "Epoch: 155, Iteration: 351, Loss: 1.4845\n",
      "Epoch: 156, Iteration: 1, Loss: 1.5077\n",
      "Epoch: 156, Iteration: 51, Loss: 1.4690\n",
      "Epoch: 156, Iteration: 101, Loss: 1.4931\n",
      "Epoch: 156, Iteration: 151, Loss: 1.4843\n",
      "Epoch: 156, Iteration: 201, Loss: 1.4767\n",
      "Epoch: 156, Iteration: 251, Loss: 1.4922\n",
      "Epoch: 156, Iteration: 301, Loss: 1.4918\n",
      "Epoch: 156, Iteration: 351, Loss: 1.5144\n",
      "Epoch: 157, Iteration: 1, Loss: 1.4842\n",
      "Epoch: 157, Iteration: 51, Loss: 1.5002\n",
      "Epoch: 157, Iteration: 101, Loss: 1.4843\n",
      "Epoch: 157, Iteration: 151, Loss: 1.4922\n",
      "Epoch: 157, Iteration: 201, Loss: 1.4849\n",
      "Epoch: 157, Iteration: 251, Loss: 1.4612\n",
      "Epoch: 157, Iteration: 301, Loss: 1.4844\n",
      "Epoch: 157, Iteration: 351, Loss: 1.5079\n",
      "Epoch: 158, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 158, Iteration: 51, Loss: 1.4846\n",
      "Epoch: 158, Iteration: 101, Loss: 1.4856\n",
      "Epoch: 158, Iteration: 151, Loss: 1.5080\n",
      "Epoch: 158, Iteration: 201, Loss: 1.4924\n",
      "Epoch: 158, Iteration: 251, Loss: 1.4922\n",
      "Epoch: 158, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 158, Iteration: 351, Loss: 1.4945\n",
      "Epoch: 159, Iteration: 1, Loss: 1.4767\n",
      "Epoch: 159, Iteration: 51, Loss: 1.4846\n",
      "Epoch: 159, Iteration: 101, Loss: 1.4766\n",
      "Epoch: 159, Iteration: 151, Loss: 1.4766\n",
      "Epoch: 159, Iteration: 201, Loss: 1.4921\n",
      "Epoch: 159, Iteration: 251, Loss: 1.4764\n",
      "Epoch: 159, Iteration: 301, Loss: 1.4701\n",
      "Epoch: 159, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 160, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 160, Iteration: 51, Loss: 1.4922\n",
      "Epoch: 160, Iteration: 101, Loss: 1.5080\n",
      "Epoch: 160, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 160, Iteration: 201, Loss: 1.5073\n",
      "Epoch: 160, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 160, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 160, Iteration: 351, Loss: 1.4694\n",
      "Epoch: 161, Iteration: 1, Loss: 1.5001\n",
      "Epoch: 161, Iteration: 51, Loss: 1.4855\n",
      "Epoch: 161, Iteration: 101, Loss: 1.5074\n",
      "Epoch: 161, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 161, Iteration: 201, Loss: 1.5077\n",
      "Epoch: 161, Iteration: 251, Loss: 1.4844\n",
      "Epoch: 161, Iteration: 301, Loss: 1.5155\n",
      "Epoch: 161, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 162, Iteration: 1, Loss: 1.5158\n",
      "Epoch: 162, Iteration: 51, Loss: 1.5054\n",
      "Epoch: 162, Iteration: 101, Loss: 1.4769\n",
      "Epoch: 162, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 162, Iteration: 201, Loss: 1.5078\n",
      "Epoch: 162, Iteration: 251, Loss: 1.5075\n",
      "Epoch: 162, Iteration: 301, Loss: 1.4690\n",
      "Epoch: 162, Iteration: 351, Loss: 1.4930\n",
      "Epoch: 163, Iteration: 1, Loss: 1.4770\n",
      "Epoch: 163, Iteration: 51, Loss: 1.4846\n",
      "Epoch: 163, Iteration: 101, Loss: 1.4924\n",
      "Epoch: 163, Iteration: 151, Loss: 1.4751\n",
      "Epoch: 163, Iteration: 201, Loss: 1.5077\n",
      "Epoch: 163, Iteration: 251, Loss: 1.4922\n",
      "Epoch: 163, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 163, Iteration: 351, Loss: 1.4843\n",
      "Epoch: 164, Iteration: 1, Loss: 1.4845\n",
      "Epoch: 164, Iteration: 51, Loss: 1.4612\n",
      "Epoch: 164, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 164, Iteration: 151, Loss: 1.4920\n",
      "Epoch: 164, Iteration: 201, Loss: 1.4923\n",
      "Epoch: 164, Iteration: 251, Loss: 1.4692\n",
      "Epoch: 164, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 164, Iteration: 351, Loss: 1.4695\n",
      "Epoch: 165, Iteration: 1, Loss: 1.4843\n",
      "Epoch: 165, Iteration: 51, Loss: 1.4844\n",
      "Epoch: 165, Iteration: 101, Loss: 1.4922\n",
      "Epoch: 165, Iteration: 151, Loss: 1.5146\n",
      "Epoch: 165, Iteration: 201, Loss: 1.4766\n",
      "Epoch: 165, Iteration: 251, Loss: 1.4764\n",
      "Epoch: 165, Iteration: 301, Loss: 1.4902\n",
      "Epoch: 165, Iteration: 351, Loss: 1.5151\n",
      "Epoch: 166, Iteration: 1, Loss: 1.4843\n",
      "Epoch: 166, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 166, Iteration: 101, Loss: 1.4851\n",
      "Epoch: 166, Iteration: 151, Loss: 1.4687\n",
      "Epoch: 166, Iteration: 201, Loss: 1.4924\n",
      "Epoch: 166, Iteration: 251, Loss: 1.5066\n",
      "Epoch: 166, Iteration: 301, Loss: 1.4997\n",
      "Epoch: 166, Iteration: 351, Loss: 1.4842\n",
      "Epoch: 167, Iteration: 1, Loss: 1.4847\n",
      "Epoch: 167, Iteration: 51, Loss: 1.4925\n",
      "Epoch: 167, Iteration: 101, Loss: 1.4921\n",
      "Epoch: 167, Iteration: 151, Loss: 1.4844\n",
      "Epoch: 167, Iteration: 201, Loss: 1.4921\n",
      "Epoch: 167, Iteration: 251, Loss: 1.4612\n",
      "Epoch: 167, Iteration: 301, Loss: 1.4690\n",
      "Epoch: 167, Iteration: 351, Loss: 1.4612\n",
      "Epoch: 168, Iteration: 1, Loss: 1.4767\n",
      "Epoch: 168, Iteration: 51, Loss: 1.4846\n",
      "Epoch: 168, Iteration: 101, Loss: 1.5077\n",
      "Epoch: 168, Iteration: 151, Loss: 1.5003\n",
      "Epoch: 168, Iteration: 201, Loss: 1.4690\n",
      "Epoch: 168, Iteration: 251, Loss: 1.4612\n",
      "Epoch: 168, Iteration: 301, Loss: 1.4844\n",
      "Epoch: 168, Iteration: 351, Loss: 1.4841\n",
      "Epoch: 169, Iteration: 1, Loss: 1.5006\n",
      "Epoch: 169, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 169, Iteration: 101, Loss: 1.4942\n",
      "Epoch: 169, Iteration: 151, Loss: 1.5082\n",
      "Epoch: 169, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 169, Iteration: 251, Loss: 1.5076\n",
      "Epoch: 169, Iteration: 301, Loss: 1.5002\n",
      "Epoch: 169, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 170, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 170, Iteration: 51, Loss: 1.4845\n",
      "Epoch: 170, Iteration: 101, Loss: 1.4923\n",
      "Epoch: 170, Iteration: 151, Loss: 1.4766\n",
      "Epoch: 170, Iteration: 201, Loss: 1.4924\n",
      "Epoch: 170, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 170, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 170, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 171, Iteration: 1, Loss: 1.4845\n",
      "Epoch: 171, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 171, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 171, Iteration: 151, Loss: 1.4999\n",
      "Epoch: 171, Iteration: 201, Loss: 1.4921\n",
      "Epoch: 171, Iteration: 251, Loss: 1.4691\n",
      "Epoch: 171, Iteration: 301, Loss: 1.5078\n",
      "Epoch: 171, Iteration: 351, Loss: 1.4845\n",
      "Epoch: 172, Iteration: 1, Loss: 1.4843\n",
      "Epoch: 172, Iteration: 51, Loss: 1.4690\n",
      "Epoch: 172, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 172, Iteration: 151, Loss: 1.4807\n",
      "Epoch: 172, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 172, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 172, Iteration: 301, Loss: 1.4690\n",
      "Epoch: 172, Iteration: 351, Loss: 1.4843\n",
      "Epoch: 173, Iteration: 1, Loss: 1.4845\n",
      "Epoch: 173, Iteration: 51, Loss: 1.4905\n",
      "Epoch: 173, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 173, Iteration: 151, Loss: 1.4924\n",
      "Epoch: 173, Iteration: 201, Loss: 1.4777\n",
      "Epoch: 173, Iteration: 251, Loss: 1.5173\n",
      "Epoch: 173, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 173, Iteration: 351, Loss: 1.4919\n",
      "Epoch: 174, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 174, Iteration: 51, Loss: 1.4923\n",
      "Epoch: 174, Iteration: 101, Loss: 1.4877\n",
      "Epoch: 174, Iteration: 151, Loss: 1.5078\n",
      "Epoch: 174, Iteration: 201, Loss: 1.4690\n",
      "Epoch: 174, Iteration: 251, Loss: 1.4927\n",
      "Epoch: 174, Iteration: 301, Loss: 1.4842\n",
      "Epoch: 174, Iteration: 351, Loss: 1.4768\n",
      "Epoch: 175, Iteration: 1, Loss: 1.4689\n",
      "Epoch: 175, Iteration: 51, Loss: 1.4766\n",
      "Epoch: 175, Iteration: 101, Loss: 1.4765\n",
      "Epoch: 175, Iteration: 151, Loss: 1.4699\n",
      "Epoch: 175, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 175, Iteration: 251, Loss: 1.4690\n",
      "Epoch: 175, Iteration: 301, Loss: 1.4612\n",
      "Epoch: 175, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 176, Iteration: 1, Loss: 1.4843\n",
      "Epoch: 176, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 176, Iteration: 101, Loss: 1.4924\n",
      "Epoch: 176, Iteration: 151, Loss: 1.4846\n",
      "Epoch: 176, Iteration: 201, Loss: 1.4690\n",
      "Epoch: 176, Iteration: 251, Loss: 1.4996\n",
      "Epoch: 176, Iteration: 301, Loss: 1.4845\n",
      "Epoch: 176, Iteration: 351, Loss: 1.4768\n",
      "Epoch: 177, Iteration: 1, Loss: 1.4888\n",
      "Epoch: 177, Iteration: 51, Loss: 1.4851\n",
      "Epoch: 177, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 177, Iteration: 151, Loss: 1.5002\n",
      "Epoch: 177, Iteration: 201, Loss: 1.5000\n",
      "Epoch: 177, Iteration: 251, Loss: 1.5075\n",
      "Epoch: 177, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 177, Iteration: 351, Loss: 1.4846\n",
      "Epoch: 178, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 178, Iteration: 51, Loss: 1.4998\n",
      "Epoch: 178, Iteration: 101, Loss: 1.4612\n",
      "Epoch: 178, Iteration: 151, Loss: 1.4922\n",
      "Epoch: 178, Iteration: 201, Loss: 1.5074\n",
      "Epoch: 178, Iteration: 251, Loss: 1.4920\n",
      "Epoch: 178, Iteration: 301, Loss: 1.4765\n",
      "Epoch: 178, Iteration: 351, Loss: 1.4844\n",
      "Epoch: 179, Iteration: 1, Loss: 1.4776\n",
      "Epoch: 179, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 179, Iteration: 101, Loss: 1.5001\n",
      "Epoch: 179, Iteration: 151, Loss: 1.5081\n",
      "Epoch: 179, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 179, Iteration: 251, Loss: 1.4924\n",
      "Epoch: 179, Iteration: 301, Loss: 1.4688\n",
      "Epoch: 179, Iteration: 351, Loss: 1.4923\n",
      "Epoch: 180, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 180, Iteration: 51, Loss: 1.4770\n",
      "Epoch: 180, Iteration: 101, Loss: 1.4998\n",
      "Epoch: 180, Iteration: 151, Loss: 1.4767\n",
      "Epoch: 180, Iteration: 201, Loss: 1.4924\n",
      "Epoch: 180, Iteration: 251, Loss: 1.4853\n",
      "Epoch: 180, Iteration: 301, Loss: 1.4767\n",
      "Epoch: 180, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 181, Iteration: 1, Loss: 1.4922\n",
      "Epoch: 181, Iteration: 51, Loss: 1.4923\n",
      "Epoch: 181, Iteration: 101, Loss: 1.5001\n",
      "Epoch: 181, Iteration: 151, Loss: 1.5153\n",
      "Epoch: 181, Iteration: 201, Loss: 1.4936\n",
      "Epoch: 181, Iteration: 251, Loss: 1.4917\n",
      "Epoch: 181, Iteration: 301, Loss: 1.4845\n",
      "Epoch: 181, Iteration: 351, Loss: 1.4921\n",
      "Epoch: 182, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 182, Iteration: 51, Loss: 1.5237\n",
      "Epoch: 182, Iteration: 101, Loss: 1.4924\n",
      "Epoch: 182, Iteration: 151, Loss: 1.4856\n",
      "Epoch: 182, Iteration: 201, Loss: 1.4924\n",
      "Epoch: 182, Iteration: 251, Loss: 1.4843\n",
      "Epoch: 182, Iteration: 301, Loss: 1.5158\n",
      "Epoch: 182, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 183, Iteration: 1, Loss: 1.5079\n",
      "Epoch: 183, Iteration: 51, Loss: 1.4845\n",
      "Epoch: 183, Iteration: 101, Loss: 1.5150\n",
      "Epoch: 183, Iteration: 151, Loss: 1.4612\n",
      "Epoch: 183, Iteration: 201, Loss: 1.4846\n",
      "Epoch: 183, Iteration: 251, Loss: 1.4766\n",
      "Epoch: 183, Iteration: 301, Loss: 1.4690\n",
      "Epoch: 183, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 184, Iteration: 1, Loss: 1.4689\n",
      "Epoch: 184, Iteration: 51, Loss: 1.4690\n",
      "Epoch: 184, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 184, Iteration: 151, Loss: 1.4845\n",
      "Epoch: 184, Iteration: 201, Loss: 1.5080\n",
      "Epoch: 184, Iteration: 251, Loss: 1.4848\n",
      "Epoch: 184, Iteration: 301, Loss: 1.5081\n",
      "Epoch: 184, Iteration: 351, Loss: 1.5003\n",
      "Epoch: 185, Iteration: 1, Loss: 1.4933\n",
      "Epoch: 185, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 185, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 185, Iteration: 151, Loss: 1.4689\n",
      "Epoch: 185, Iteration: 201, Loss: 1.5234\n",
      "Epoch: 185, Iteration: 251, Loss: 1.4846\n",
      "Epoch: 185, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 185, Iteration: 351, Loss: 1.4924\n",
      "Epoch: 186, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 186, Iteration: 51, Loss: 1.4922\n",
      "Epoch: 186, Iteration: 101, Loss: 1.5230\n",
      "Epoch: 186, Iteration: 151, Loss: 1.4844\n",
      "Epoch: 186, Iteration: 201, Loss: 1.4847\n",
      "Epoch: 186, Iteration: 251, Loss: 1.4920\n",
      "Epoch: 186, Iteration: 301, Loss: 1.4765\n",
      "Epoch: 186, Iteration: 351, Loss: 1.4843\n",
      "Epoch: 187, Iteration: 1, Loss: 1.4871\n",
      "Epoch: 187, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 187, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 187, Iteration: 151, Loss: 1.4768\n",
      "Epoch: 187, Iteration: 201, Loss: 1.5002\n",
      "Epoch: 187, Iteration: 251, Loss: 1.5079\n",
      "Epoch: 187, Iteration: 301, Loss: 1.4922\n",
      "Epoch: 187, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 188, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 188, Iteration: 51, Loss: 1.4924\n",
      "Epoch: 188, Iteration: 101, Loss: 1.4925\n",
      "Epoch: 188, Iteration: 151, Loss: 1.4690\n",
      "Epoch: 188, Iteration: 201, Loss: 1.5001\n",
      "Epoch: 188, Iteration: 251, Loss: 1.4767\n",
      "Epoch: 188, Iteration: 301, Loss: 1.4843\n",
      "Epoch: 188, Iteration: 351, Loss: 1.4922\n",
      "Epoch: 189, Iteration: 1, Loss: 1.4997\n",
      "Epoch: 189, Iteration: 51, Loss: 1.5235\n",
      "Epoch: 189, Iteration: 101, Loss: 1.4845\n",
      "Epoch: 189, Iteration: 151, Loss: 1.5000\n",
      "Epoch: 189, Iteration: 201, Loss: 1.5183\n",
      "Epoch: 189, Iteration: 251, Loss: 1.4765\n",
      "Epoch: 189, Iteration: 301, Loss: 1.4767\n",
      "Epoch: 189, Iteration: 351, Loss: 1.5079\n",
      "Epoch: 190, Iteration: 1, Loss: 1.4924\n",
      "Epoch: 190, Iteration: 51, Loss: 1.4768\n",
      "Epoch: 190, Iteration: 101, Loss: 1.4612\n",
      "Epoch: 190, Iteration: 151, Loss: 1.5079\n",
      "Epoch: 190, Iteration: 201, Loss: 1.4688\n",
      "Epoch: 190, Iteration: 251, Loss: 1.4612\n",
      "Epoch: 190, Iteration: 301, Loss: 1.5076\n",
      "Epoch: 190, Iteration: 351, Loss: 1.4845\n",
      "Epoch: 191, Iteration: 1, Loss: 1.4919\n",
      "Epoch: 191, Iteration: 51, Loss: 1.4922\n",
      "Epoch: 191, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 191, Iteration: 151, Loss: 1.5002\n",
      "Epoch: 191, Iteration: 201, Loss: 1.4689\n",
      "Epoch: 191, Iteration: 251, Loss: 1.4843\n",
      "Epoch: 191, Iteration: 301, Loss: 1.4844\n",
      "Epoch: 191, Iteration: 351, Loss: 1.4923\n",
      "Epoch: 192, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 192, Iteration: 51, Loss: 1.4690\n",
      "Epoch: 192, Iteration: 101, Loss: 1.4998\n",
      "Epoch: 192, Iteration: 151, Loss: 1.5002\n",
      "Epoch: 192, Iteration: 201, Loss: 1.4690\n",
      "Epoch: 192, Iteration: 251, Loss: 1.4770\n",
      "Epoch: 192, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 192, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 193, Iteration: 1, Loss: 1.4765\n",
      "Epoch: 193, Iteration: 51, Loss: 1.4902\n",
      "Epoch: 193, Iteration: 101, Loss: 1.4688\n",
      "Epoch: 193, Iteration: 151, Loss: 1.4924\n",
      "Epoch: 193, Iteration: 201, Loss: 1.4765\n",
      "Epoch: 193, Iteration: 251, Loss: 1.5001\n",
      "Epoch: 193, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 193, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 194, Iteration: 1, Loss: 1.4918\n",
      "Epoch: 194, Iteration: 51, Loss: 1.4846\n",
      "Epoch: 194, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 194, Iteration: 151, Loss: 1.4687\n",
      "Epoch: 194, Iteration: 201, Loss: 1.4844\n",
      "Epoch: 194, Iteration: 251, Loss: 1.5075\n",
      "Epoch: 194, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 194, Iteration: 351, Loss: 1.4922\n",
      "Epoch: 195, Iteration: 1, Loss: 1.4690\n",
      "Epoch: 195, Iteration: 51, Loss: 1.4844\n",
      "Epoch: 195, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 195, Iteration: 151, Loss: 1.5002\n",
      "Epoch: 195, Iteration: 201, Loss: 1.4768\n",
      "Epoch: 195, Iteration: 251, Loss: 1.4766\n",
      "Epoch: 195, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 195, Iteration: 351, Loss: 1.4766\n",
      "Epoch: 196, Iteration: 1, Loss: 1.4924\n",
      "Epoch: 196, Iteration: 51, Loss: 1.4921\n",
      "Epoch: 196, Iteration: 101, Loss: 1.4768\n",
      "Epoch: 196, Iteration: 151, Loss: 1.4919\n",
      "Epoch: 196, Iteration: 201, Loss: 1.4921\n",
      "Epoch: 196, Iteration: 251, Loss: 1.4847\n",
      "Epoch: 196, Iteration: 301, Loss: 1.4924\n",
      "Epoch: 196, Iteration: 351, Loss: 1.4843\n",
      "Epoch: 197, Iteration: 1, Loss: 1.4844\n",
      "Epoch: 197, Iteration: 51, Loss: 1.5022\n",
      "Epoch: 197, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 197, Iteration: 151, Loss: 1.4999\n",
      "Epoch: 197, Iteration: 201, Loss: 1.4842\n",
      "Epoch: 197, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 197, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 197, Iteration: 351, Loss: 1.4998\n",
      "Epoch: 198, Iteration: 1, Loss: 1.4768\n",
      "Epoch: 198, Iteration: 51, Loss: 1.5078\n",
      "Epoch: 198, Iteration: 101, Loss: 1.4767\n",
      "Epoch: 198, Iteration: 151, Loss: 1.4765\n",
      "Epoch: 198, Iteration: 201, Loss: 1.4843\n",
      "Epoch: 198, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 198, Iteration: 301, Loss: 1.4768\n",
      "Epoch: 198, Iteration: 351, Loss: 1.4690\n",
      "Epoch: 199, Iteration: 1, Loss: 1.4846\n",
      "Epoch: 199, Iteration: 51, Loss: 1.4690\n",
      "Epoch: 199, Iteration: 101, Loss: 1.4846\n",
      "Epoch: 199, Iteration: 151, Loss: 1.4841\n",
      "Epoch: 199, Iteration: 201, Loss: 1.4769\n",
      "Epoch: 199, Iteration: 251, Loss: 1.4768\n",
      "Epoch: 199, Iteration: 301, Loss: 1.4784\n",
      "Epoch: 199, Iteration: 351, Loss: 1.5157\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50 // total_client_num\n",
    "for epoch in range(200):\n",
    "    \n",
    "    # Iterate over multiple clients\n",
    "\n",
    "      # All clients have their own data source and we are using a single trainloader here for illustration of the simulated setup\n",
    "#         for i, data in enumerate(client_trainloader[client_num], 0):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        activations = []\n",
    "        client_optimizers = []\n",
    "        for client_num in range(total_client_num):\n",
    "#             print(\"Current active client is {}\".format(client_num))\n",
    "            client = client_model_list[client_num]\n",
    "            client_optimizer = client_optimizer_list[client_num]\n",
    "            client.train()\n",
    "            client_optimizer.zero_grad()\n",
    "            activation = client(inputs)\n",
    "            # Logic to load the weights from the previous client\n",
    "            activations.append(activation) \n",
    "            client_optimizers.append(client_optimizer)\n",
    "            \n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        server_optimizer.zero_grad()\n",
    "\n",
    "        # Client part\n",
    "\n",
    "        server_inputs = torch.mean(torch.stack(activations), dim=0).detach().clone()\n",
    "\n",
    "        #Server part\n",
    "        server_inputs = Variable(server_inputs, requires_grad=True)\n",
    "        outputs = server(server_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        server_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_samples += labels.shape[0]\n",
    "        for activation, client_optimizer in zip(activations, client_optimizers):\n",
    "            # Client part\n",
    "            activation.backward(server_inputs.grad)\n",
    "            client_optimizer.step()\n",
    "\n",
    "        if i % 50 == 1:\n",
    "            print('Epoch: {}, Iteration: {}, Loss: {:.4f}'.format(epoch, i, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 8, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(activations), dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xBWBYavjcC8v",
    "outputId": "d8cb340d-6b4d-4014-f21e-b91a63b67780",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 50 // total_client_num\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Iterate over multiple clients\n",
    "    for client_num in range(total_client_num):\n",
    "        print(\"Current active client is {}\".format(client_num))\n",
    "        client = client_model_list[client_num]\n",
    "        client_optimizer = client_optimizer_list[client_num]\n",
    "        # Logic to load the weights from the previous client\n",
    "        print('client_num', client_num)\n",
    "        if client_num == 0:\n",
    "            if epoch != 0:\n",
    "                prev_client = total_client_num - 1\n",
    "                prev_client_weights = client_model_list[prev_client].state_dict()\n",
    "                client.load_state_dict(prev_client_weights)\n",
    "                print(\"Loaded client {}'s weight successfully\".format(prev_client))\n",
    "        else:\n",
    "            prev_client = client_num - 1\n",
    "            prev_client_weights = client_model_list[prev_client].state_dict()\n",
    "            client.load_state_dict(prev_client_weights)\n",
    "            print(\"Loaded client {}'s weight successfully\".format(prev_client))\n",
    "\n",
    "        client.train()\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "      # All clients have their own data source and we are using a single trainloader here for illustration of the simulated setup\n",
    "#         for i, data in enumerate(client_trainloader[client_num], 0):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            client_optimizer.zero_grad()\n",
    "            server_optimizer.zero_grad()\n",
    "            \n",
    "            # Client part\n",
    "            activations = client(inputs)\n",
    "            server_inputs = activations.detach().clone()\n",
    "            \n",
    "            #Server part\n",
    "            server_inputs = Variable(server_inputs, requires_grad=True)\n",
    "            outputs = server(server_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            server_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            total_samples += labels.shape[0]\n",
    "            # Client part\n",
    "            activations.backward(server_inputs.grad)\n",
    "            client_optimizer.step()\n",
    "          \n",
    "            if i % 50 == 1:\n",
    "                print('Client: {} Epoch: {}, Iteration: {}, Loss: {:.4f}'.format(client_num, epoch, i, running_loss/(i+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.mkdir('saved_models_splitNN')\n",
    "client_model_path = \"./saved_models_splitNN/trained_client_model_499_epoch.pt\"\n",
    "server_model_path = \"./saved_models_splitNN/trained_server_model_499_epoch.pt\"\n",
    "torch.save(client.state_dict(), client_model_path)\n",
    "torch.save(server.state_dict(), server_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyWPyZ2zaiQd"
   },
   "source": [
    "# Future Directions\n",
    "\n",
    "1. How to make the round robin protocol asynchronous?\n",
    "2. Different data distribution across clients?\n",
    "3. Different topologies of client models, multiple servers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SplitLearningPart-1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "fed-benchmark",
   "language": "python",
   "name": "fed-benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ed019f1ca5e44f480a48ed25d1668c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "323542ddb63649fb8fb410e4f7038ac5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59c0dc8a0c7d4b4690436e0bc43de692",
      "placeholder": "​",
      "style": "IPY_MODEL_5862037b575c49d384703c30d7103322",
      "value": " 170500096/? [00:20&lt;00:00, 51281318.88it/s]"
     }
    },
    "3e365a51059d4681983f5894dc733e50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4187546d5cc0494685f29befc0279976": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "572768a4ac4640c09b6abb920b64d20c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4187546d5cc0494685f29befc0279976",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e365a51059d4681983f5894dc733e50",
      "value": 1
     }
    },
    "5862037b575c49d384703c30d7103322": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59c0dc8a0c7d4b4690436e0bc43de692": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dde44056ded14b1da781f381cd68531c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_572768a4ac4640c09b6abb920b64d20c",
       "IPY_MODEL_323542ddb63649fb8fb410e4f7038ac5"
      ],
      "layout": "IPY_MODEL_0ed019f1ca5e44f480a48ed25d1668c0"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
